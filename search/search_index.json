{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Causal Component Analysis (CauCA) Overview Causal Component Analysis is a project that bridges the gap between Independent Component Analysis (ICA) and Causal Representation Learning (CRL). This project includes implementations and experiments related to the papers: 1. > Liang, W., Keki\u0107, A., von K\u00fcgelgen, J., Buchholz, S., Besserve, M., Gresele, L., & Sch\u00f6lkopf, B. (2023). Causal Component Analysis . In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems. The corrsponding experiments are in the [experiments/cauca](experiments/cauca/README.md) folder. von K\u00fcgelgen, J., Besserve, M., Liang, W., Gresele, L., Keki\u0107, A., Bareinboim, E., Blei, D., & Sch\u00f6lkopf, B. (2023). Nonparametric Identifiability of Causal Representations from Unknown Interventions . In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems. The corrsponding experiments are in the experiments/nonparam_ident folder. Installation Clone the repository git clone git@github.com:akekic/causal-component-analysis.git and install the package pip install -e . License This project is licensed under the MIT license . See the LICENSE file for details. Citation If you use CauCA , please cite the corresponding paper as follows. Liang, W., Keki\u0107, A., von K\u00fcgelgen, J., Buchholz, S., Besserve, M., Gresele, L., & Sch\u00f6lkopf, B. (2023). Causal Component Analysis. In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems. Bibtex @inproceedings{ liang2023causal, title={Causal Component Analysis}, author={Wendong Liang and Armin Keki{\\'c} and Julius von K{\\\"u}gelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch{\\\"o}lkopf}, booktitle={Thirty-seventh Conference on Neural Information Processing Systems}, year={2023}, url={https://openreview.net/forum?id=HszLRiHyfO} }","title":"About"},{"location":"#causal-component-analysis-cauca","text":"","title":"Causal Component Analysis (CauCA)"},{"location":"#overview","text":"Causal Component Analysis is a project that bridges the gap between Independent Component Analysis (ICA) and Causal Representation Learning (CRL). This project includes implementations and experiments related to the papers: 1. > Liang, W., Keki\u0107, A., von K\u00fcgelgen, J., Buchholz, S., Besserve, M., Gresele, L., & Sch\u00f6lkopf, B. (2023). Causal Component Analysis . In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems. The corrsponding experiments are in the [experiments/cauca](experiments/cauca/README.md) folder. von K\u00fcgelgen, J., Besserve, M., Liang, W., Gresele, L., Keki\u0107, A., Bareinboim, E., Blei, D., & Sch\u00f6lkopf, B. (2023). Nonparametric Identifiability of Causal Representations from Unknown Interventions . In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems. The corrsponding experiments are in the experiments/nonparam_ident folder.","title":"Overview"},{"location":"#installation","text":"Clone the repository git clone git@github.com:akekic/causal-component-analysis.git and install the package pip install -e .","title":"Installation"},{"location":"#license","text":"This project is licensed under the MIT license . See the LICENSE file for details.","title":"License"},{"location":"#citation","text":"If you use CauCA , please cite the corresponding paper as follows. Liang, W., Keki\u0107, A., von K\u00fcgelgen, J., Buchholz, S., Besserve, M., Gresele, L., & Sch\u00f6lkopf, B. (2023). Causal Component Analysis. In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems. Bibtex @inproceedings{ liang2023causal, title={Causal Component Analysis}, author={Wendong Liang and Armin Keki{\\'c} and Julius von K{\\\"u}gelgen and Simon Buchholz and Michel Besserve and Luigi Gresele and Bernhard Sch{\\\"o}lkopf}, booktitle={Thirty-seventh Conference on Neural Information Processing Systems}, year={2023}, url={https://openreview.net/forum?id=HszLRiHyfO} }","title":"Citation"},{"location":"references/","text":"API references data_module MultiEnvDataModule Bases: LightningDataModule Data module for multi-environment data. Attributes medgp: MultiEnvDGP Multi-environment data generating process. num_samples_per_env: int Number of samples per environment. batch_size: int Batch size. num_workers: int Number of workers for the data loaders. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on. log_dir: Optional[Path] Directory to save summary statistics and plots to. Default: None. intervention_target_misspec: bool Whether to misspecify the intervention targets. If true, the intervention targets are permuted. I.e. the model received the wrong intervention targets. Default: False. intervention_target_perm: Optional[list[int]] Permutation of the intervention targets. If None, a random permutation is used. Only used if intervention_target_misspec is True. Default: None. Methods setup(stage=None) -> None Setup the data module. This is where the data is sampled. train_dataloader() -> DataLoader Return the training data loader. val_dataloader() -> DataLoader Return the validation data loader. test_dataloader() -> DataLoader Return the test data loader. Source code in data_generator/data_module.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class MultiEnvDataModule ( LightningDataModule ): \"\"\" Data module for multi-environment data. Attributes ---------- medgp: MultiEnvDGP Multi-environment data generating process. num_samples_per_env: int Number of samples per environment. batch_size: int Batch size. num_workers: int Number of workers for the data loaders. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on. log_dir: Optional[Path] Directory to save summary statistics and plots to. Default: None. intervention_target_misspec: bool Whether to misspecify the intervention targets. If true, the intervention targets are permuted. I.e. the model received the wrong intervention targets. Default: False. intervention_target_perm: Optional[list[int]] Permutation of the intervention targets. If None, a random permutation is used. Only used if intervention_target_misspec is True. Default: None. Methods ------- setup(stage=None) -> None Setup the data module. This is where the data is sampled. train_dataloader() -> DataLoader Return the training data loader. val_dataloader() -> DataLoader Return the validation data loader. test_dataloader() -> DataLoader Return the test data loader. \"\"\" def __init__ ( self , multi_env_dgp : MultiEnvDGP , num_samples_per_env : int , batch_size : int , num_workers : int , intervention_targets_per_env : Tensor , log_dir : Optional [ Path ] = None , intervention_target_misspec : bool = False , intervention_target_perm : Optional [ list [ int ]] = None , ) -> None : super () . __init__ () self . medgp = multi_env_dgp self . num_samples_per_env = num_samples_per_env self . batch_size = batch_size self . num_workers = num_workers self . intervention_targets_per_env = intervention_targets_per_env self . log_dir = log_dir self . intervention_target_misspec = intervention_target_misspec latent_dim = self . medgp . latent_scm . latent_dim assert ( intervention_target_perm is None or len ( intervention_target_perm ) == latent_dim ) self . intervention_target_perm = intervention_target_perm def setup ( self , stage : Optional [ str ] = None ) -> None : latent_dim = self . medgp . latent_scm . latent_dim num_envs = self . intervention_targets_per_env . shape [ 0 ] x , v , u , e , intervention_targets , log_prob = self . medgp . sample ( self . num_samples_per_env , intervention_targets_per_env = self . intervention_targets_per_env , ) if self . intervention_target_misspec : assert ( num_envs == latent_dim + 1 ), \"only works if num_envs == num_causal_variables + 1\" if self . intervention_target_perm is None : perm = random_perm ( latent_dim ) self . intervention_target_perm = perm else : perm = self . intervention_target_perm # remember where old targets were idx_mask_list = [] for i in range ( latent_dim ): idx_mask = intervention_targets [:, i ] == 1 idx_mask_list . append ( idx_mask ) intervention_targets [ idx_mask , i ] = 0 # permute targets for i in range ( latent_dim ): intervention_targets [ idx_mask_list [ i ], perm [ i ]] = 1 dataset = TensorDataset ( x , v , u , e , intervention_targets , log_prob ) train_size = int ( 0.8 * len ( dataset )) val_size = int ( 0.5 * ( len ( dataset ) - train_size )) test_size = len ( dataset ) - train_size - val_size ( self . train_dataset , self . val_dataset , self . test_dataset , ) = torch . utils . data . random_split ( dataset , [ train_size , val_size , test_size ]) if self . log_dir is not None : self . log_dir . mkdir ( parents = True , exist_ok = True ) summary_stats = summary_statistics ( x , v , e , intervention_targets ) for key , value in summary_stats . items (): value . to_csv ( self . log_dir / f \" { key } _summary_stats.csv\" ) plot_dag ( self . medgp . adjacency_matrix , self . log_dir ) try : with open ( self . log_dir / \"base_coeff_values.txt\" , \"w\" ) as f : f . write ( str ( self . medgp . latent_scm . base_coeff_values )) except AttributeError : pass # save mixing function coefficients self . medgp . mixing_function . save_coeffs ( self . log_dir ) def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers , ) def val_dataloader ( self ) -> DataLoader : val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers , ) return val_loader def test_dataloader ( self ) -> DataLoader : test_loader = DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers , ) return test_loader graph_sampler sample_random_dag ( n_nodes , edge_prob ) Sample a random DAG with n_nodes nodes and edge_prob probability of an edge between two nodes. We ensure that there is at least one edge in the graph by rejecting graphs with no edges and resampling. Parameters n_nodes: int Number of nodes in the graph. edge_prob: float Probability of an edge between two nodes. Returns adjaceny_matrix: np.ndarray, shape (n_nodes, n_nodes) The adjacency matrix of the sampled DAG. Source code in data_generator/graph_sampler.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def sample_random_dag ( n_nodes : int , edge_prob : float ) -> np . ndarray : \"\"\" Sample a random DAG with n_nodes nodes and edge_prob probability of an edge between two nodes. We ensure that there is at least one edge in the graph by rejecting graphs with no edges and resampling. Parameters ---------- n_nodes: int Number of nodes in the graph. edge_prob: float Probability of an edge between two nodes. Returns ------- adjaceny_matrix: np.ndarray, shape (n_nodes, n_nodes) The adjacency matrix of the sampled DAG. \"\"\" while True : adjaceny_matrix = np . random . binomial ( 1 , edge_prob , size = ( n_nodes , n_nodes )) # put all lower triangular elements to zero adjaceny_matrix [ np . tril_indices ( n_nodes )] = 0 # make sure the graph has at least one edge if np . sum ( adjaceny_matrix ) > 0 : break return adjaceny_matrix mixing_function LinearMixing Bases: MixingFunction Linear mixing function. The coefficients are sampled from a uniform distribution. Parameters latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. Source code in data_generator/mixing_function.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class LinearMixing ( MixingFunction ): \"\"\" Linear mixing function. The coefficients are sampled from a uniform distribution. Parameters ---------- latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. \"\"\" def __init__ ( self , latent_dim : int , observation_dim : int ) -> None : super () . __init__ ( latent_dim , observation_dim ) self . coeffs = torch . rand (( latent_dim , observation_dim )) def __call__ ( self , v : Tensor ) -> Tensor : return torch . matmul ( v , self . coeffs . to ( v . device )) def save_coeffs ( self , path : Path ) -> None : # save matrix coefficients torch . save ( self . coeffs , path / \"matrix.pt\" ) matrix_np = self . coeffs . numpy () # convert to Numpy array df = pd . DataFrame ( matrix_np ) # convert to a dataframe df . to_csv ( path / \"matrix.csv\" , index = False ) # save as csv MixingFunction Bases: ABC Base class for mixing functions. The mixing function is the function that maps from the latent space to the observation space. Parameters latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. Source code in data_generator/mixing_function.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class MixingFunction ( ABC ): \"\"\" Base class for mixing functions. The mixing function is the function that maps from the latent space to the observation space. Parameters ---------- latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. \"\"\" def __init__ ( self , latent_dim : int , observation_dim : int ) -> None : self . latent_dim = latent_dim self . observation_dim = observation_dim def __call__ ( self , v : Tensor ) -> Tensor : \"\"\" Apply the mixing function to the latent variables. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- x: Tensor, shape (num_samples, observation_dim) Observed variables. \"\"\" raise NotImplementedError () def save_coeffs ( self , path : Path ) -> None : \"\"\" Save the coefficients of the mixing function to disk. Parameters ---------- path: Path Path to save the coefficients to. \"\"\" raise NotImplementedError () def unmixing_jacobian ( self , v : Tensor ) -> Tensor : \"\"\" Compute the jacobian of the inverse mixing function using autograd and the inverse function theorem. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- unmixing_jacobian: Tensor, shape (num_samples, observation_dim, latent_dim) Jacobian of the inverse mixing function. References ---------- https://en.wikipedia.org/wiki/Inverse_function_theorem https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/7 \"\"\" func = self . __call__ inputs = v mixing_jacobian = torch . vmap ( torch . func . jacrev ( func ))( inputs ) unmixing_jacobian = torch . inverse ( mixing_jacobian ) return unmixing_jacobian __call__ ( v ) Apply the mixing function to the latent variables. Parameters v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns x: Tensor, shape (num_samples, observation_dim) Observed variables. Source code in data_generator/mixing_function.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __call__ ( self , v : Tensor ) -> Tensor : \"\"\" Apply the mixing function to the latent variables. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- x: Tensor, shape (num_samples, observation_dim) Observed variables. \"\"\" raise NotImplementedError () save_coeffs ( path ) Save the coefficients of the mixing function to disk. Parameters path: Path Path to save the coefficients to. Source code in data_generator/mixing_function.py 45 46 47 48 49 50 51 52 53 54 def save_coeffs ( self , path : Path ) -> None : \"\"\" Save the coefficients of the mixing function to disk. Parameters ---------- path: Path Path to save the coefficients to. \"\"\" raise NotImplementedError () unmixing_jacobian ( v ) Compute the jacobian of the inverse mixing function using autograd and the inverse function theorem. Parameters v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns unmixing_jacobian: Tensor, shape (num_samples, observation_dim, latent_dim) Jacobian of the inverse mixing function. References https://en.wikipedia.org/wiki/Inverse_function_theorem https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/7 Source code in data_generator/mixing_function.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def unmixing_jacobian ( self , v : Tensor ) -> Tensor : \"\"\" Compute the jacobian of the inverse mixing function using autograd and the inverse function theorem. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- unmixing_jacobian: Tensor, shape (num_samples, observation_dim, latent_dim) Jacobian of the inverse mixing function. References ---------- https://en.wikipedia.org/wiki/Inverse_function_theorem https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/7 \"\"\" func = self . __call__ inputs = v mixing_jacobian = torch . vmap ( torch . func . jacrev ( func ))( inputs ) unmixing_jacobian = torch . inverse ( mixing_jacobian ) return unmixing_jacobian NonlinearMixing Bases: MixingFunction Nonlinear mixing function. The function is composed of a number of invertible matrices and leaky-tanh nonlinearities. I.e. we apply a random neural network to the latent variables. Parameters latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. n_nonlinearities: int Number of layers (i.e. invertible maps and nonlinearities) in the mixing function. Default: 1. Source code in data_generator/mixing_function.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class NonlinearMixing ( MixingFunction ): \"\"\" Nonlinear mixing function. The function is composed of a number of invertible matrices and leaky-tanh nonlinearities. I.e. we apply a random neural network to the latent variables. Parameters ---------- latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. n_nonlinearities: int Number of layers (i.e. invertible maps and nonlinearities) in the mixing function. Default: 1. \"\"\" def __init__ ( self , latent_dim : int , observation_dim : int , n_nonlinearities : int = 1 ) -> None : super () . __init__ ( latent_dim , observation_dim ) assert latent_dim == observation_dim self . coefs = torch . rand (( latent_dim , observation_dim )) self . n_nonlinearities = n_nonlinearities matrices = [] for i in range ( n_nonlinearities ): matrices . append ( sample_invertible_matrix ( observation_dim )) self . matrices = matrices nonlinearities = [] for i in range ( n_nonlinearities ): nonlinearities . append ( leaky_tanh ) self . nonlinearities = nonlinearities def __call__ ( self , v : Tensor ) -> Tensor : x = v for i in range ( self . n_nonlinearities ): mat = self . matrices [ i ] . to ( v . device ) nonlinearity = self . nonlinearities [ i ] x = nonlinearity ( torch . matmul ( x , mat )) return x def save_coeffs ( self , path : Path ) -> None : # save matrix coefficients for i in range ( self . n_nonlinearities ): torch . save ( self . matrices [ i ], path / f \"matrix_ { i } .pt\" ) matrix_np = self . matrices [ i ] . numpy () # convert to Numpy array df = pd . DataFrame ( matrix_np ) # convert to a dataframe df . to_csv ( path / f \"matrix_ { i } .csv\" , index = False ) # save as csv # save matrix determinants in one csv matrix_determinants = [] for i in range ( self . n_nonlinearities ): matrix_determinants . append ( torch . det ( self . matrices [ i ])) matrix_determinants_np = torch . stack ( matrix_determinants ) . numpy () df = pd . DataFrame ( matrix_determinants_np ) df . to_csv ( path / \"matrix_determinants.csv\" ) multi_env_gdp MultiEnvDGP Multi-environment data generating process (DGP). The DGP is defined by a latent structural causal model (SCM), a noise generator and a mixing function. This class is used to generate data from those three components. The latent SCM is a multi-environment SCM, i.e. it generates data for multiple environments which differ by interventions on some of the variables. The noise generator is also multi-environmental, i.e. it generates noise for multiple environments. The mixing function is a function that maps the latent variables to the observed variables. The mixing function is the same for all environments. Attributes mixing_function: MixingFunction Mixing function. latent_scm: MultiEnvLatentSCM Multi-environment latent SCM. noise_generator: MultiEnvNoise Multi-environment noise generator. Methods sample(num_samples_per_env, intervention_targets_per_env) -> tuple[Tensor, ...] Sample from the DGP. Source code in data_generator/multi_env_gdp.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class MultiEnvDGP : \"\"\" Multi-environment data generating process (DGP). The DGP is defined by a latent structural causal model (SCM), a noise generator and a mixing function. This class is used to generate data from those three components. The latent SCM is a multi-environment SCM, i.e. it generates data for multiple environments which differ by interventions on some of the variables. The noise generator is also multi-environmental, i.e. it generates noise for multiple environments. The mixing function is a function that maps the latent variables to the observed variables. The mixing function is the same for all environments. Attributes ---------- mixing_function: MixingFunction Mixing function. latent_scm: MultiEnvLatentSCM Multi-environment latent SCM. noise_generator: MultiEnvNoise Multi-environment noise generator. Methods ------- sample(num_samples_per_env, intervention_targets_per_env) -> tuple[Tensor, ...] Sample from the DGP. \"\"\" def __init__ ( self , mixing_function : MixingFunction , latent_scm : MultiEnvLatentSCM , noise_generator : MultiEnvNoise , ) -> None : self . mixing_function = mixing_function self . latent_scm = latent_scm self . noise_generator = noise_generator self . adjacency_matrix = self . latent_scm . adjacency_matrix def sample ( self , num_samples_per_env : int , intervention_targets_per_env : Tensor , ) -> tuple [ Tensor , ... ]: \"\"\" Sample from the DGP. Parameters ---------- num_samples_per_env: int Number of samples to generate per environment. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. Returns ------- x: Tensor, shape (num_samples_per_env * num_envs, observation_dim) Samples of observed variables. v: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of latent variables. u: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of exogenous noise variables. e: Tensor, shape (num_samples_per_env * num_envs, 1) Environment indicator. intervention_targets: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Intervention targets. log_prob: Tensor, shape (num_samples_per_env * num_envs, 1) Ground-truth log probability of the samples. \"\"\" num_envs = intervention_targets_per_env . shape [ 0 ] shape = ( num_samples_per_env , num_envs , self . latent_scm . latent_dim , ) u = torch . zeros ( shape ) v = torch . zeros ( shape ) intervention_targets_out = torch . zeros ( shape ) e = torch . zeros (( num_samples_per_env , num_envs , 1 ), dtype = torch . long ) log_prob = torch . zeros (( num_samples_per_env , num_envs , 1 )) for env in range ( num_envs ): int_targets_env = intervention_targets_per_env [ env , :] noise_samples_env = self . noise_generator . sample ( env , size = num_samples_per_env ) noise_log_prob_env = self . noise_generator . log_prob ( noise_samples_env , env ) latent_samples_env = self . latent_scm . push_forward ( noise_samples_env , env ) log_det_scm = self . latent_scm . log_inverse_jacobian ( latent_samples_env , noise_samples_env , env ) intervention_targets_out [:, env , :] = int_targets_env u [:, env , :] = noise_samples_env v [:, env , :] = latent_samples_env e [:, env , :] = env log_prob [:, env , :] = ( log_det_scm + noise_log_prob_env . sum ( dim = 1 ) ) . unsqueeze ( 1 ) flattened_shape = ( num_samples_per_env * num_envs , self . latent_scm . latent_dim ) intervention_targets_out = intervention_targets_out . reshape ( flattened_shape ) u = u . reshape ( flattened_shape ) v = v . reshape ( flattened_shape ) e = e . reshape ( num_samples_per_env * num_envs , 1 ) log_prob = log_prob . reshape ( num_samples_per_env * num_envs , 1 ) x = self . mixing_function ( v ) unmixing_jacobian = self . mixing_function . unmixing_jacobian ( v ) log_det_unmixing_jacobian = torch . slogdet ( unmixing_jacobian ) . logabsdet . unsqueeze ( 1 ) log_prob += log_det_unmixing_jacobian return ( x , v , u , e , intervention_targets_out , log_prob , ) sample ( num_samples_per_env , intervention_targets_per_env ) Sample from the DGP. Parameters num_samples_per_env: int Number of samples to generate per environment. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. Returns x: Tensor, shape (num_samples_per_env * num_envs, observation_dim) Samples of observed variables. v: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of latent variables. u: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of exogenous noise variables. e: Tensor, shape (num_samples_per_env * num_envs, 1) Environment indicator. intervention_targets: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Intervention targets. log_prob: Tensor, shape (num_samples_per_env * num_envs, 1) Ground-truth log probability of the samples. Source code in data_generator/multi_env_gdp.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def sample ( self , num_samples_per_env : int , intervention_targets_per_env : Tensor , ) -> tuple [ Tensor , ... ]: \"\"\" Sample from the DGP. Parameters ---------- num_samples_per_env: int Number of samples to generate per environment. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. Returns ------- x: Tensor, shape (num_samples_per_env * num_envs, observation_dim) Samples of observed variables. v: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of latent variables. u: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of exogenous noise variables. e: Tensor, shape (num_samples_per_env * num_envs, 1) Environment indicator. intervention_targets: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Intervention targets. log_prob: Tensor, shape (num_samples_per_env * num_envs, 1) Ground-truth log probability of the samples. \"\"\" num_envs = intervention_targets_per_env . shape [ 0 ] shape = ( num_samples_per_env , num_envs , self . latent_scm . latent_dim , ) u = torch . zeros ( shape ) v = torch . zeros ( shape ) intervention_targets_out = torch . zeros ( shape ) e = torch . zeros (( num_samples_per_env , num_envs , 1 ), dtype = torch . long ) log_prob = torch . zeros (( num_samples_per_env , num_envs , 1 )) for env in range ( num_envs ): int_targets_env = intervention_targets_per_env [ env , :] noise_samples_env = self . noise_generator . sample ( env , size = num_samples_per_env ) noise_log_prob_env = self . noise_generator . log_prob ( noise_samples_env , env ) latent_samples_env = self . latent_scm . push_forward ( noise_samples_env , env ) log_det_scm = self . latent_scm . log_inverse_jacobian ( latent_samples_env , noise_samples_env , env ) intervention_targets_out [:, env , :] = int_targets_env u [:, env , :] = noise_samples_env v [:, env , :] = latent_samples_env e [:, env , :] = env log_prob [:, env , :] = ( log_det_scm + noise_log_prob_env . sum ( dim = 1 ) ) . unsqueeze ( 1 ) flattened_shape = ( num_samples_per_env * num_envs , self . latent_scm . latent_dim ) intervention_targets_out = intervention_targets_out . reshape ( flattened_shape ) u = u . reshape ( flattened_shape ) v = v . reshape ( flattened_shape ) e = e . reshape ( num_samples_per_env * num_envs , 1 ) log_prob = log_prob . reshape ( num_samples_per_env * num_envs , 1 ) x = self . mixing_function ( v ) unmixing_jacobian = self . mixing_function . unmixing_jacobian ( v ) log_det_unmixing_jacobian = torch . slogdet ( unmixing_jacobian ) . logabsdet . unsqueeze ( 1 ) log_prob += log_det_unmixing_jacobian return ( x , v , u , e , intervention_targets_out , log_prob , ) make_multi_env_dgp ( latent_dim , observation_dim , adjacency_matrix , intervention_targets_per_env , shift_noise = True , noise_shift_type = 'mean' , mixing = 'nonlinear' , scm = 'linear' , n_nonlinearities = 1 , scm_coeffs_low =- 1 , scm_coeffs_high = 1 , coeffs_min_abs_value = None , edge_prob = None , snr = 1.0 ) Create a multi-environment data generating process (DGP). Parameters latent_dim: int Dimension of the latent variables. observation_dim: int Dimension of the observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent SCM. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. shift_noise: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. noise_shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". mixing: str Mixing function. Options: \"linear\" or \"nonlinear\". Default: \"nonlinear\". scm: str Latent SCM. Options: \"linear\" or \"location-scale\". Default: \"linear\". n_nonlinearities: int Number of nonlinearities in the nonlinear mixing function. Default: 1. scm_coeffs_low: float Lower bound of the SCM coefficients in linear SCMs. Default: -1. scm_coeffs_high: float Upper bound of the SCM coefficients in linear SCMs. Default: 1. coeffs_min_abs_value: float Minimum absolute value of the SCM coefficients in linear SCMs. If None, no minimum absolute value is enforced. Default: None. edge_prob: float Probability of an edge in the adjacency matrix if no adjacency matrix is given. Default: None. snr: float Signal-to-noise ratio of the location-scale SCM. Default: 1.0. Returns medgp: MultiEnvDGP Multi-environment data generating process. Source code in data_generator/multi_env_gdp.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def make_multi_env_dgp ( latent_dim : int , observation_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , shift_noise : bool = True , noise_shift_type : str = \"mean\" , mixing : str = \"nonlinear\" , scm : str = \"linear\" , n_nonlinearities : int = 1 , scm_coeffs_low : float = - 1 , scm_coeffs_high : float = 1 , coeffs_min_abs_value : float = None , edge_prob : float = None , snr : float = 1.0 , ) -> MultiEnvDGP : \"\"\" Create a multi-environment data generating process (DGP). Parameters ---------- latent_dim: int Dimension of the latent variables. observation_dim: int Dimension of the observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent SCM. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. shift_noise: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. noise_shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". mixing: str Mixing function. Options: \"linear\" or \"nonlinear\". Default: \"nonlinear\". scm: str Latent SCM. Options: \"linear\" or \"location-scale\". Default: \"linear\". n_nonlinearities: int Number of nonlinearities in the nonlinear mixing function. Default: 1. scm_coeffs_low: float Lower bound of the SCM coefficients in linear SCMs. Default: -1. scm_coeffs_high: float Upper bound of the SCM coefficients in linear SCMs. Default: 1. coeffs_min_abs_value: float Minimum absolute value of the SCM coefficients in linear SCMs. If None, no minimum absolute value is enforced. Default: None. edge_prob: float Probability of an edge in the adjacency matrix if no adjacency matrix is given. Default: None. snr: float Signal-to-noise ratio of the location-scale SCM. Default: 1.0. Returns ------- medgp: MultiEnvDGP Multi-environment data generating process. \"\"\" if mixing == \"linear\" : mixing_function = LinearMixing ( latent_dim = latent_dim , observation_dim = observation_dim ) elif mixing == \"nonlinear\" : mixing_function = NonlinearMixing ( latent_dim = latent_dim , observation_dim = observation_dim , n_nonlinearities = n_nonlinearities , ) else : raise ValueError ( f \"Unknown mixing function { mixing } \" ) # if adjacency_matrix is not given as numpy array, sample a random one if not isinstance ( adjacency_matrix , np . ndarray ): assert ( edge_prob is not None ), \"edge_prob must be given if no adjacency_matrix is given\" adjacency_matrix = sample_random_dag ( latent_dim , edge_prob ) adjacency_matrix = adjacency_matrix if scm == \"linear\" : latent_scm = LinearSCM ( adjacency_matrix = adjacency_matrix , latent_dim = latent_dim , intervention_targets_per_env = intervention_targets_per_env , coeffs_low = scm_coeffs_low , coeffs_high = scm_coeffs_high , coeffs_min_abs_value = coeffs_min_abs_value , ) elif scm == \"location-scale\" : latent_scm = LocationScaleSCM ( adjacency_matrix = adjacency_matrix , latent_dim = latent_dim , intervention_targets_per_env = intervention_targets_per_env , snr = snr , ) else : raise ValueError ( f \"Unknown SCM { scm } \" ) noise_generator = GaussianNoise ( latent_dim = latent_dim , intervention_targets_per_env = intervention_targets_per_env , shift = shift_noise , shift_type = noise_shift_type , ) medgp = MultiEnvDGP ( latent_scm = latent_scm , noise_generator = noise_generator , mixing_function = mixing_function , ) return medgp noise_generator MultiEnvNoise Bases: ABC Base class for multi-environment noise generators. Attributes latent_dim: int Latent dimension. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. mean: float Mean of the noise distribution. If shift is True and shift_type is \"mean\", the mean of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 0.0. std: float Standard deviation of the noise distribution. If shift is True and shift_type is \"std\", the standard deviation of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 1.0. shift: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". Methods sample(e, size=1) -> Tensor Sample from the noise distribution for a given environment. Source code in data_generator/noise_generator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class MultiEnvNoise ( ABC ): \"\"\" Base class for multi-environment noise generators. Attributes ---------- latent_dim: int Latent dimension. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. mean: float Mean of the noise distribution. If shift is True and shift_type is \"mean\", the mean of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 0.0. std: float Standard deviation of the noise distribution. If shift is True and shift_type is \"std\", the standard deviation of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 1.0. shift: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". Methods ------- sample(e, size=1) -> Tensor Sample from the noise distribution for a given environment. \"\"\" def __init__ ( self , latent_dim : int , intervention_targets_per_env : Tensor , mean : float = 0.0 , std : float = 1.0 , shift : bool = False , shift_type : str = \"mean\" , ) -> None : self . latent_dim = latent_dim self . intervention_targets = intervention_targets_per_env self . mean = mean self . std = std self . shift = shift assert shift_type in [ \"mean\" , \"std\" ], f \"Invalid shift type: { shift_type } \" self . shift_type = shift_type def sample ( self , e : int , size : int = 1 ) -> Tensor : \"\"\" Sample from the noise distribution for a given environment. Parameters ---------- e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. size: int Number of samples to generate. Default: 1. Returns ------- Tensor, shape (size, latent_dim) Samples from the noise distribution. \"\"\" raise NotImplementedError () def log_prob ( self , u : Tensor , e : int ) -> Tensor : \"\"\" Compute the log probability of u under the noise distribution for a given environment. We assume that all samples come from the same environment. Parameters ---------- u: Tensor, shape (size, latent_dim) Samples from the noise distribution. e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. Returns ------- log_prob: Tensor, shape (size, latent_dim) Log probability of u. \"\"\" raise NotImplementedError () log_prob ( u , e ) Compute the log probability of u under the noise distribution for a given environment. We assume that all samples come from the same environment. Parameters u: Tensor, shape (size, latent_dim) Samples from the noise distribution. e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. Returns log_prob: Tensor, shape (size, latent_dim) Log probability of u. Source code in data_generator/noise_generator.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def log_prob ( self , u : Tensor , e : int ) -> Tensor : \"\"\" Compute the log probability of u under the noise distribution for a given environment. We assume that all samples come from the same environment. Parameters ---------- u: Tensor, shape (size, latent_dim) Samples from the noise distribution. e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. Returns ------- log_prob: Tensor, shape (size, latent_dim) Log probability of u. \"\"\" raise NotImplementedError () sample ( e , size = 1 ) Sample from the noise distribution for a given environment. Parameters e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. size: int Number of samples to generate. Default: 1. Returns Tensor, shape (size, latent_dim) Samples from the noise distribution. Source code in data_generator/noise_generator.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def sample ( self , e : int , size : int = 1 ) -> Tensor : \"\"\" Sample from the noise distribution for a given environment. Parameters ---------- e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. size: int Number of samples to generate. Default: 1. Returns ------- Tensor, shape (size, latent_dim) Samples from the noise distribution. \"\"\" raise NotImplementedError () scm LinearSCM Bases: MultiEnvLatentSCM Multi-environment latent SCM, where all causal mechanisms are linear. The coefficients of the linear causal mechanisms are sampled from a uniform distribution. Inherits all attributes and methods from MultiEnvLatentSCM. Additional attributes coeffs_low : float Lower bound for the coefficients of the linear causal mechanisms. Default: -1.0. coeffs_high : float Upper bound for the coefficients of the linear causal mechanisms. Default: 1.0. coeffs_min_abs_value : Optional[float] Minimum absolute value for the coefficients of the linear causal mechanisms. If None, no minimum absolute value is enforced. Default: None. Additional methods setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Source code in data_generator/scm.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 class LinearSCM ( MultiEnvLatentSCM ): \"\"\" Multi-environment latent SCM, where all causal mechanisms are linear. The coefficients of the linear causal mechanisms are sampled from a uniform distribution. Inherits all attributes and methods from MultiEnvLatentSCM. Additional attributes --------------------- coeffs_low : float Lower bound for the coefficients of the linear causal mechanisms. Default: -1.0. coeffs_high : float Upper bound for the coefficients of the linear causal mechanisms. Default: 1.0. coeffs_min_abs_value : Optional[float] Minimum absolute value for the coefficients of the linear causal mechanisms. If None, no minimum absolute value is enforced. Default: None. Additional methods ------------------ setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , coeffs_low : float = - 1.0 , coeffs_high : float = 1.0 , coeffs_min_abs_value : Optional [ float ] = None , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) coeffs_low: float coeffs_high: float coeffs_min_abs_value: Optional[float] \"\"\" super () . __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , ) self . coeffs_low = coeffs_low self . coeffs_high = coeffs_high self . coeffs_min_abs_value = coeffs_min_abs_value base_functions = [] base_inverse_jac = [] base_coeff_values = [] for index in range ( self . latent_dim ): parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = sample_coeffs ( self . coeffs_low , self . coeffs_high , ( len ( parents ) + 1 ,), min_abs_value = self . coeffs_min_abs_value , ) coeffs [ - 1 ] = 1 # set the noise coefficient to 1 base_functions . append ( partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs ) ) base_inverse_jac . append ( partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) ) base_coeff_values . append ( coeffs ) self . base_functions = base_functions self . base_inverse_jac = base_inverse_jac self . base_coeff_values = base_coeff_values self . functions_per_env , self . inverse_jac_per_env = self . setup_functions_per_env ( intervention_targets_per_env ) def setup_functions_per_env ( self , intervention_targets_per_env : Tensor ) -> tuple [ dict [ int , callable ], dict [ int , callable ]]: \"\"\" Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Parameters ---------- intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets for each environment. Returns ------- functions_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. \"\"\" functions_per_env = {} inverse_jac_per_env = {} num_envs = intervention_targets_per_env . shape [ 0 ] for env in range ( num_envs ): functions_env = {} inverse_jac_env = {} for index in self . topological_order : if intervention_targets_per_env [ env ][ index ] == 1 : parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = torch . zeros (( len ( parents ) + 1 ,)) # cut edges from parents coeffs [ - 1 ] = 1.0 # still use noise f = partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs , ) inverse_jac = partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) else : f = self . base_functions [ index ] inverse_jac = self . base_inverse_jac [ index ] functions_env [ index ] = f inverse_jac_env [ index ] = inverse_jac functions_per_env [ env ] = functions_env inverse_jac_per_env [ env ] = inverse_jac_env return functions_per_env , inverse_jac_per_env __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , coeffs_low =- 1.0 , coeffs_high = 1.0 , coeffs_min_abs_value = None ) Parameters adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) coeffs_low: float coeffs_high: float coeffs_min_abs_value: Optional[float] Source code in data_generator/scm.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , coeffs_low : float = - 1.0 , coeffs_high : float = 1.0 , coeffs_min_abs_value : Optional [ float ] = None , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) coeffs_low: float coeffs_high: float coeffs_min_abs_value: Optional[float] \"\"\" super () . __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , ) self . coeffs_low = coeffs_low self . coeffs_high = coeffs_high self . coeffs_min_abs_value = coeffs_min_abs_value base_functions = [] base_inverse_jac = [] base_coeff_values = [] for index in range ( self . latent_dim ): parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = sample_coeffs ( self . coeffs_low , self . coeffs_high , ( len ( parents ) + 1 ,), min_abs_value = self . coeffs_min_abs_value , ) coeffs [ - 1 ] = 1 # set the noise coefficient to 1 base_functions . append ( partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs ) ) base_inverse_jac . append ( partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) ) base_coeff_values . append ( coeffs ) self . base_functions = base_functions self . base_inverse_jac = base_inverse_jac self . base_coeff_values = base_coeff_values self . functions_per_env , self . inverse_jac_per_env = self . setup_functions_per_env ( intervention_targets_per_env ) setup_functions_per_env ( intervention_targets_per_env ) Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Parameters intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets for each environment. Returns functions_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. Source code in data_generator/scm.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def setup_functions_per_env ( self , intervention_targets_per_env : Tensor ) -> tuple [ dict [ int , callable ], dict [ int , callable ]]: \"\"\" Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Parameters ---------- intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets for each environment. Returns ------- functions_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. \"\"\" functions_per_env = {} inverse_jac_per_env = {} num_envs = intervention_targets_per_env . shape [ 0 ] for env in range ( num_envs ): functions_env = {} inverse_jac_env = {} for index in self . topological_order : if intervention_targets_per_env [ env ][ index ] == 1 : parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = torch . zeros (( len ( parents ) + 1 ,)) # cut edges from parents coeffs [ - 1 ] = 1.0 # still use noise f = partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs , ) inverse_jac = partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) else : f = self . base_functions [ index ] inverse_jac = self . base_inverse_jac [ index ] functions_env [ index ] = f inverse_jac_env [ index ] = inverse_jac functions_per_env [ env ] = functions_env inverse_jac_per_env [ env ] = inverse_jac_env return functions_per_env , inverse_jac_per_env LocationScaleSCM Bases: MultiEnvLatentSCM Multi-environment latent SCM, where all causal mechanisms are location-scale functions [1] of the form v_i = snr * f_loc(pa_i) + f_scale(u_i), where f_loc and f_scale are random nonlinear functions, pa_i are the parents of v_i, and u_i is the exogenous noise variable corresponding to v_i. snr is the signal-to-noise ratio. Inherits all attributes and methods from MultiEnvLatentSCM. Additional attributes n_nonlinearities : int Number of nonlinearities in the location-scale functions. Default: 3. snr : float Signal-to-noise ratio. Default: 1.0. base_functions : list[callable] List of base functions that implement the location-scale functions for each latent variable in the unintervened (observational) environment. base_inverse_jac : list[callable] List of base functions that implement the log of the inverse Jacobian of the location-scale functions for each latent variable in the unintervened (observational) environment. Additional methods setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the causal mechanisms based on the location-scale functions are defined. References [1] https://en.wikipedia.org/wiki/Location%E2%80%93scale_family Source code in data_generator/scm.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 class LocationScaleSCM ( MultiEnvLatentSCM ): \"\"\" Multi-environment latent SCM, where all causal mechanisms are location-scale functions [1] of the form v_i = snr * f_loc(pa_i) + f_scale(u_i), where f_loc and f_scale are random nonlinear functions, pa_i are the parents of v_i, and u_i is the exogenous noise variable corresponding to v_i. snr is the signal-to-noise ratio. Inherits all attributes and methods from MultiEnvLatentSCM. Additional attributes --------------------- n_nonlinearities : int Number of nonlinearities in the location-scale functions. Default: 3. snr : float Signal-to-noise ratio. Default: 1.0. base_functions : list[callable] List of base functions that implement the location-scale functions for each latent variable in the unintervened (observational) environment. base_inverse_jac : list[callable] List of base functions that implement the log of the inverse Jacobian of the location-scale functions for each latent variable in the unintervened (observational) environment. Additional methods ------------------ setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the causal mechanisms based on the location-scale functions are defined. References ---------- [1] https://en.wikipedia.org/wiki/Location%E2%80%93scale_family \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , n_nonlinearities : int = 3 , snr : float = 1.0 , ) -> None : super () . __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , ) self . n_nonlinearities = n_nonlinearities self . snr = snr base_functions = [] base_inverse_jac = [] for index in range ( self . latent_dim ): parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) f , inverse_jac = make_location_scale_function ( index , parents , n_nonlinearities , snr ) base_functions . append ( f ) base_inverse_jac . append ( inverse_jac ) self . base_functions = base_functions self . base_inverse_jac = base_inverse_jac self . functions_per_env , self . inverse_jac_per_env = self . setup_functions_per_env ( intervention_targets_per_env ) def setup_functions_per_env ( self , intervention_targets_per_env : Tensor ) -> tuple [ dict [ int , callable ], dict [ int , callable ]]: functions_per_env = {} inverse_jac_per_env = {} num_envs = intervention_targets_per_env . shape [ 0 ] for env in range ( num_envs ): functions_env = {} inverse_jac_env = {} for index in self . topological_order : if intervention_targets_per_env [ env ][ index ] == 1 : parents = [] f , inverse_jac = make_location_scale_function ( index , parents , self . n_nonlinearities , self . snr ) else : f = self . base_functions [ index ] inverse_jac = self . base_inverse_jac [ index ] functions_env [ index ] = f inverse_jac_env [ index ] = inverse_jac functions_per_env [ env ] = functions_env inverse_jac_per_env [ env ] = inverse_jac_env return functions_per_env , inverse_jac_per_env MultiEnvLatentSCM Bases: ABC Base class for multi-environment latent SCM. In environments where a variable is intervened on, the dependencies of the variable are cut. Note that this class only implements the causal mechanisms. The exogenous noise variables, which mayb also shift under interventions, are implemented in the noise generator. Attributes adjacency_matrix : np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the SCM. latent_dim : int Dimension of the latent space. intervention_targets_per_env : Tensor, shape (num_envs, latent_dim) Binary tensor indicating which variables are intervened on in each environment. dag : nx.DiGraph Directed acyclic graph representing the causal structure. topological_order : list[int] Topological order of the causal graph. functions_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. Methods push_forward(u: Tensor, env: int) -> Tensor Push forward the latent variable u through the SCM in environment env. log_inverse_jacobian(v: Tensor, u: Tensor, env: int) -> Tensor Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Source code in data_generator/scm.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class MultiEnvLatentSCM ( ABC ): \"\"\" Base class for multi-environment latent SCM. In environments where a variable is intervened on, the dependencies of the variable are cut. Note that this class only implements the causal mechanisms. The exogenous noise variables, which mayb also shift under interventions, are implemented in the noise generator. Attributes ---------- adjacency_matrix : np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the SCM. latent_dim : int Dimension of the latent space. intervention_targets_per_env : Tensor, shape (num_envs, latent_dim) Binary tensor indicating which variables are intervened on in each environment. dag : nx.DiGraph Directed acyclic graph representing the causal structure. topological_order : list[int] Topological order of the causal graph. functions_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. Methods ------- push_forward(u: Tensor, env: int) -> Tensor Push forward the latent variable u through the SCM in environment env. log_inverse_jacobian(v: Tensor, u: Tensor, env: int) -> Tensor Compute the log of the inverse Jacobian of the SCM in environment env at v and u. \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) \"\"\" self . adjacency_matrix = adjacency_matrix self . latent_dim = latent_dim self . intervention_targets_per_env = intervention_targets_per_env assert adjacency_matrix . shape [ 0 ] == adjacency_matrix . shape [ 1 ] == latent_dim self . dag = nx . DiGraph ( adjacency_matrix ) self . topological_order = list ( nx . topological_sort ( self . dag )) self . functions_per_env = None self . inverse_jac_per_env = None def push_forward ( self , u : Tensor , env : int ) -> Tensor : \"\"\" Push forward the latent variable u through the SCM in environment env. Parameters ---------- u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. \"\"\" v = torch . nan * torch . zeros_like ( u ) for index in self . topological_order : f = self . functions_per_env [ env ][ index ] v [:, index ] = f ( v , u ) return v def log_inverse_jacobian ( self , v : Tensor , u : Tensor , env : int ) -> Tensor : \"\"\" Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- log_inv_jac: Tensor, shape (num_samples,) Log of the inverse Jacobian of the SCM at v and u. \"\"\" log_inv_jac = 0.0 for index in self . topological_order : log_inv_jac += torch . log ( self . inverse_jac_per_env [ env ][ index ]( v , u )) return log_inv_jac __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env ) Parameters adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Source code in data_generator/scm.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) \"\"\" self . adjacency_matrix = adjacency_matrix self . latent_dim = latent_dim self . intervention_targets_per_env = intervention_targets_per_env assert adjacency_matrix . shape [ 0 ] == adjacency_matrix . shape [ 1 ] == latent_dim self . dag = nx . DiGraph ( adjacency_matrix ) self . topological_order = list ( nx . topological_sort ( self . dag )) self . functions_per_env = None self . inverse_jac_per_env = None log_inverse_jacobian ( v , u , env ) Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Parameters v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns log_inv_jac: Tensor, shape (num_samples,) Log of the inverse Jacobian of the SCM at v and u. Source code in data_generator/scm.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def log_inverse_jacobian ( self , v : Tensor , u : Tensor , env : int ) -> Tensor : \"\"\" Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- log_inv_jac: Tensor, shape (num_samples,) Log of the inverse Jacobian of the SCM at v and u. \"\"\" log_inv_jac = 0.0 for index in self . topological_order : log_inv_jac += torch . log ( self . inverse_jac_per_env [ env ][ index ]( v , u )) return log_inv_jac push_forward ( u , env ) Push forward the latent variable u through the SCM in environment env. Parameters u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. Source code in data_generator/scm.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def push_forward ( self , u : Tensor , env : int ) -> Tensor : \"\"\" Push forward the latent variable u through the SCM in environment env. Parameters ---------- u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. \"\"\" v = torch . nan * torch . zeros_like ( u ) for index in self . topological_order : f = self . functions_per_env [ env ][ index ] v [:, index ] = f ( v , u ) return v cauca_model CauCAModel Bases: LightningModule , ABC Base class for Causal Component Analysis (CauCA) models. It implements the training loop and the evaluation metrics. Attributes latent_dim : int Dimensionality of the latent space. adjacency_matrix : np.ndarray, shape (num_nodes, num_nodes) Adjacency matrix of the causal graph assumed by the model. This is not necessarily the true adjacency matrix of the data generating process (see below). adjacency_matrix_gt : np.ndarray, shape (num_nodes, num_nodes) Ground truth adjacency matrix of the causal graph. This is the adjacency matrix of the data generating process. adjacency_misspecified : bool Whether the adjacency matrix is misspecified. If True, the model assumes a wrong adjacency matrix. lr : float Learning rate for the optimizer. weight_decay : float Weight decay for the optimizer. lr_scheduler : str Learning rate scheduler to use. If None, no scheduler is used. Options are \"cosine\" or None. Default: None. lr_min : float Minimum learning rate for the scheduler. Default: 0.0. encoder : CauCAEncoder The CauCA encoder. Needs to be set in subclasses. Methods training_step(batch, batch_idx) -> Tensor Training step. validation_step(batch, batch_idx) -> dict[str, Tensor] Validation step: basically passes data to validation_epoch_end. validation_epoch_end(outputs) -> None Computes validation metrics across all validation data. test_step(batch, batch_idx) -> dict[str, Tensor] Test step: basically passes data to test_epoch_end. test_epoch_end(outputs) -> None Computes test metrics across all test data. configure_optimizers() -> dict | torch.optim.Optimizer Configures the optimizer and learning rate scheduler. forward(x) -> torch.Tensor Computes the latent variables from the observed data. on_before_optimizer_step(optimizer, optimizer_idx) -> None Callback that is called before each optimizer step. It ensures that some gradients are set to zero to fix some causal mechanisms. See documentation of ParamMultiEnvCausalDistribution for more details. set_adjacency(adjacency_matrix, adjacency_misspecified) -> np.ndarray Sets the adjacency matrix and possibly changes it if it is misspecified. Source code in model/cauca_model.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 class CauCAModel ( pl . LightningModule , ABC ): \"\"\" Base class for Causal Component Analysis (CauCA) models. It implements the training loop and the evaluation metrics. Attributes ---------- latent_dim : int Dimensionality of the latent space. adjacency_matrix : np.ndarray, shape (num_nodes, num_nodes) Adjacency matrix of the causal graph assumed by the model. This is not necessarily the true adjacency matrix of the data generating process (see below). adjacency_matrix_gt : np.ndarray, shape (num_nodes, num_nodes) Ground truth adjacency matrix of the causal graph. This is the adjacency matrix of the data generating process. adjacency_misspecified : bool Whether the adjacency matrix is misspecified. If True, the model assumes a wrong adjacency matrix. lr : float Learning rate for the optimizer. weight_decay : float Weight decay for the optimizer. lr_scheduler : str Learning rate scheduler to use. If None, no scheduler is used. Options are \"cosine\" or None. Default: None. lr_min : float Minimum learning rate for the scheduler. Default: 0.0. encoder : CauCAEncoder The CauCA encoder. Needs to be set in subclasses. Methods ------- training_step(batch, batch_idx) -> Tensor Training step. validation_step(batch, batch_idx) -> dict[str, Tensor] Validation step: basically passes data to validation_epoch_end. validation_epoch_end(outputs) -> None Computes validation metrics across all validation data. test_step(batch, batch_idx) -> dict[str, Tensor] Test step: basically passes data to test_epoch_end. test_epoch_end(outputs) -> None Computes test metrics across all test data. configure_optimizers() -> dict | torch.optim.Optimizer Configures the optimizer and learning rate scheduler. forward(x) -> torch.Tensor Computes the latent variables from the observed data. on_before_optimizer_step(optimizer, optimizer_idx) -> None Callback that is called before each optimizer step. It ensures that some gradients are set to zero to fix some causal mechanisms. See documentation of ParamMultiEnvCausalDistribution for more details. set_adjacency(adjacency_matrix, adjacency_misspecified) -> np.ndarray Sets the adjacency matrix and possibly changes it if it is misspecified. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , ) -> None : super () . __init__ () self . latent_dim = latent_dim self . adjacency_matrix = self . set_adjacency ( adjacency_matrix , adjacency_misspecified ) self . adjacency_matrix_gt = adjacency_matrix self . adjacency_misspecified = adjacency_misspecified self . lr = lr self . weight_decay = weight_decay self . lr_scheduler = lr_scheduler self . lr_min = lr_min self . encoder = None # needs to be set in subclasses self . save_hyperparameters () @staticmethod def set_adjacency ( adjacency_matrix : np . ndarray , adjacency_misspecified : bool ) -> np . ndarray : if not adjacency_misspecified : return adjacency_matrix if adjacency_matrix . shape [ 0 ] == 2 and np . sum ( adjacency_matrix ) == 0 : # for 2 variables, if adjacency matrix is [[0, 0], [0, 0]], then # replace with [[0, 1], [0, 0]] adjacency_matrix_out = np . zeros_like ( adjacency_matrix ) adjacency_matrix_out [ 0 , 1 ] = 1 return adjacency_matrix_out elif adjacency_matrix . shape [ 0 ] > 2 : raise ValueError ( \"Adjacency misspecification not supported for empty adjacency matrix for >2 variables\" ) else : return adjacency_matrix . T def training_step ( self , batch : tuple [ Tensor , ... ], batch_idx : int ) -> Tensor : x , v , u , e , int_target , log_prob_gt = batch log_prob , res = self . encoder . multi_env_log_prob ( x , e , int_target ) loss = - log_prob . mean () self . log ( \"train_loss\" , loss , prog_bar = False ) return loss def validation_step ( self , batch : tuple [ Tensor , ... ], batch_idx : int ) -> dict [ str , Tensor ]: x , v , u , e , int_target , log_prob_gt = batch log_prob , res = self . encoder . multi_env_log_prob ( x , e , int_target ) v_hat = self ( x ) return { \"log_prob\" : log_prob , \"log_prob_gt\" : log_prob_gt , \"v\" : v , \"v_hat\" : v_hat , } def validation_epoch_end ( self , outputs : List [ dict ]) -> None : log_prob = torch . cat ([ o [ \"log_prob\" ] for o in outputs ]) log_prob_gt = torch . cat ([ o [ \"log_prob_gt\" ] for o in outputs ]) v = torch . cat ([ o [ \"v\" ] for o in outputs ]) v_hat = torch . cat ([ o [ \"v_hat\" ] for o in outputs ]) mcc = mean_correlation_coefficient ( v_hat , v ) mcc_spearman = mean_correlation_coefficient ( v_hat , v , method = \"spearman\" ) loss = - log_prob . mean () loss_gt = - log_prob_gt . mean () self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_loss_gt\" , loss_gt , prog_bar = False ) self . log ( \"val_mcc\" , mcc . mean (), prog_bar = True ) for i , mcc_value in enumerate ( mcc ): self . log ( f \"val_mcc_ { i } \" , mcc_value , prog_bar = False ) self . log ( \"val_mcc_spearman\" , mcc_spearman . mean (), prog_bar = True ) for i , mcc_value in enumerate ( mcc_spearman ): self . log ( f \"val_mcc_spearman_ { i } \" , mcc_value , prog_bar = False ) def test_step ( self , batch : tuple [ Tensor , ... ], batch_idx : int ) -> Union [ None , dict [ str , Tensor ]]: x , v , u , e , int_target , log_prob_gt = batch log_prob , res = self . encoder . multi_env_log_prob ( x , e , int_target ) return { \"log_prob\" : log_prob , \"v\" : v , \"v_hat\" : self ( x ), } def test_epoch_end ( self , outputs : List [ dict ]) -> None : log_prob = torch . cat ([ o [ \"log_prob\" ] for o in outputs ]) loss = - log_prob . mean () v = torch . cat ([ o [ \"v\" ] for o in outputs ]) v_hat = torch . cat ([ o [ \"v_hat\" ] for o in outputs ]) mcc = mean_correlation_coefficient ( v_hat , v ) self . log ( \"test_loss\" , loss , prog_bar = False ) self . log ( \"test_mcc\" , mcc . mean (), prog_bar = False ) for i , mcc_value in enumerate ( mcc ): self . log ( f \"test_mcc_ { i } \" , mcc_value , prog_bar = False ) def configure_optimizers ( self ) -> dict | torch . optim . Optimizer : config_dict = {} optimizer = torch . optim . Adam ( self . parameters (), lr = self . lr , weight_decay = self . weight_decay ) config_dict [ \"optimizer\" ] = optimizer if self . lr_scheduler == \"cosine\" : # cosine learning rate annealing lr_scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = self . trainer . max_epochs , eta_min = self . lr_min , verbose = True , ) lr_scheduler_config = { \"scheduler\" : lr_scheduler , \"interval\" : \"epoch\" , } config_dict [ \"lr_scheduler\" ] = lr_scheduler_config elif self . lr_scheduler is None : return optimizer else : raise ValueError ( f \"Unknown lr_scheduler: { self . lr_scheduler } \" ) return config_dict def forward ( self , x : torch . Tensor ) -> torch . Tensor : v_hat = self . encoder ( x ) return v_hat def on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None : num_envs = len ( self . encoder . intervention_targets_per_env ) num_vars = self . adjacency_matrix . shape [ 0 ] # set gradients to fixed q0 parameters to zero if self . encoder . q0 . trainable : try : for param_idx , ( env , i ) in enumerate ( product ( range ( num_envs ), range ( num_vars )) ): if not self . encoder . q0 . noise_means_requires_grad [ env ][ i ]: list ( self . encoder . q0 . noise_means . parameters ())[ param_idx ] . grad = None if not self . encoder . q0 . noise_stds_requires_grad [ env ][ i ]: list ( self . encoder . q0 . noise_stds . parameters ())[ param_idx ] . grad = None except AttributeError : pass LinearCauCAModel Bases: CauCAModel CauCA model with linear unmixing function. Source code in model/cauca_model.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 class LinearCauCAModel ( CauCAModel ): \"\"\" CauCA model with linear unmixing function. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , fix_mechanisms : bool = True , nonparametric_base_distr : bool = False , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , lr = lr , weight_decay = weight_decay , lr_scheduler = lr_scheduler , lr_min = lr_min , adjacency_misspecified = adjacency_misspecified , ) self . encoder = LinearCauCAEncoder ( latent_dim , self . adjacency_matrix , # this is the misspecified adjacency matrix if adjacency_misspecified=True intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , nonparametric_base_distr = nonparametric_base_distr , ) self . save_hyperparameters () NaiveNonlinearModel Bases: CauCAModel Naive CauCA model with nonlinear unmixing function. It assumes no causal dependencies. Source code in model/cauca_model.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 class NaiveNonlinearModel ( CauCAModel ): \"\"\" Naive CauCA model with nonlinear unmixing function. It assumes no causal dependencies. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , k_flows : int = 1 , intervention_targets_per_env : Optional [ torch . Tensor ] = None , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , lr = lr , weight_decay = weight_decay , lr_scheduler = lr_scheduler , lr_min = lr_min , adjacency_misspecified = adjacency_misspecified , ) self . encoder = NaiveEncoder ( latent_dim , self . adjacency_matrix , # this is the misspecified adjacency matrix if adjacency_misspecified=True K = k_flows , intervention_targets_per_env = intervention_targets_per_env , net_hidden_dim = net_hidden_dim , net_hidden_layers = net_hidden_layers , ) self . save_hyperparameters () NonlinearCauCAModel Bases: CauCAModel CauCA model with nonlinear unmixing function. Additional attributes k_flows : int Number of flows to use in the nonlinear unmixing function. Default: 1. net_hidden_dim : int Hidden dimension of the neural network used in the nonlinear unmixing function. Default: 128. net_hidden_layers : int Number of hidden layers of the neural network used in the nonlinear unmixing function. Default: 3. fix_mechanisms : bool Some mechanisms can be fixed to a simple gaussian distribution without loss of generality. This has only an effect for the parametric base distribution. If True, these mechanisms are fixed. Default: True. fix_all_intervention_targets : bool When fixable mechanisms are fixed, this parameter determines whether all intervention targets are fixed (option 1) or all intervention targets which are non-root nodes together with all non-intervened root nodes (option 2). See documentation of ParamMultiEnvCausalDistribution for more details. Default: False. nonparametric_base_distr : bool Whether to use a nonparametric base distribution for the flows. If false, a parametric linear gaussian causal base distribution is used. Default: False. K_cbn : int Number of flows to use in the nonlinear nonparametric base distribution. Default: 3. net_hidden_dim_cbn : int Hidden dimension of the neural network used in the nonlinear nonparametric base distribution. Default: 128. net_hidden_layers_cbn : int Number of hidden layers of the neural network used in the nonlinear nonparametric base distribution. Default: 3. Source code in model/cauca_model.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class NonlinearCauCAModel ( CauCAModel ): \"\"\" CauCA model with nonlinear unmixing function. Additional attributes --------------------- k_flows : int Number of flows to use in the nonlinear unmixing function. Default: 1. net_hidden_dim : int Hidden dimension of the neural network used in the nonlinear unmixing function. Default: 128. net_hidden_layers : int Number of hidden layers of the neural network used in the nonlinear unmixing function. Default: 3. fix_mechanisms : bool Some mechanisms can be fixed to a simple gaussian distribution without loss of generality. This has only an effect for the parametric base distribution. If True, these mechanisms are fixed. Default: True. fix_all_intervention_targets : bool When fixable mechanisms are fixed, this parameter determines whether all intervention targets are fixed (option 1) or all intervention targets which are non-root nodes together with all non-intervened root nodes (option 2). See documentation of ParamMultiEnvCausalDistribution for more details. Default: False. nonparametric_base_distr : bool Whether to use a nonparametric base distribution for the flows. If false, a parametric linear gaussian causal base distribution is used. Default: False. K_cbn : int Number of flows to use in the nonlinear nonparametric base distribution. Default: 3. net_hidden_dim_cbn : int Hidden dimension of the neural network used in the nonlinear nonparametric base distribution. Default: 128. net_hidden_layers_cbn : int Number of hidden layers of the neural network used in the nonlinear nonparametric base distribution. Default: 3. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , k_flows : int = 1 , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , fix_mechanisms : bool = True , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , K_cbn : int = 3 , net_hidden_dim_cbn : int = 128 , net_hidden_layers_cbn : int = 3 , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , lr = lr , weight_decay = weight_decay , lr_scheduler = lr_scheduler , lr_min = lr_min , adjacency_misspecified = adjacency_misspecified , ) self . encoder = NonlinearCauCAEncoder ( latent_dim , self . adjacency_matrix , # this is the misspecified adjacency matrix if adjacency_misspecified=True K = k_flows , intervention_targets_per_env = intervention_targets_per_env , net_hidden_dim = net_hidden_dim , net_hidden_layers = net_hidden_layers , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , nonparametric_base_distr = nonparametric_base_distr , K_cbn = K_cbn , net_hidden_dim_cbn = net_hidden_dim_cbn , net_hidden_layers_cbn = net_hidden_layers_cbn , ) self . save_hyperparameters () encoder CauCAEncoder Bases: NormalizingFlow CauCA encoder for multi-environment data. The encoder maps from the observed data x to the latent space v_hat. The latent space is assumed to have causal structure. The encoder is trained to maximize the likelihood of the data under the causal model. x and v_hat are assumed to have the same dimension. The encoder has two main components A causal base distribution q0 over the latent space. This encodes the latent causal structure. An unmixing function mapping from the observations to the latent space. Attributes latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. fix_mechanisms: bool Whether to fix some fixable mechanisms in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. fix_all_intervention_targets: bool Whether to fix all intervention targets in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. nonparametric_base_distr: bool Whether to use a nonparametric base distribution. If False, a parametric base distribution assuming linear causal mechanisms is used. Default: False. flows: Optional[list[nf.flows.Flow]] List of normalizing flows to use for the unmixing function. Default: None. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. Methods multi_env_log_prob(x, e, intervention_targets) -> Tensor Computes log probability of x in environment e. forward(x) -> Tensor Maps from the observed data x to the latent space v_hat. Source code in model/encoder.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class CauCAEncoder ( nf . NormalizingFlow ): \"\"\" CauCA encoder for multi-environment data. The encoder maps from the observed data x to the latent space v_hat. The latent space is assumed to have causal structure. The encoder is trained to maximize the likelihood of the data under the causal model. x and v_hat are assumed to have the same dimension. The encoder has two main components: 1. A causal base distribution q0 over the latent space. This encodes the latent causal structure. 2. An unmixing function mapping from the observations to the latent space. Attributes ---------- latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. fix_mechanisms: bool Whether to fix some fixable mechanisms in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. fix_all_intervention_targets: bool Whether to fix all intervention targets in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. nonparametric_base_distr: bool Whether to use a nonparametric base distribution. If False, a parametric base distribution assuming linear causal mechanisms is used. Default: False. flows: Optional[list[nf.flows.Flow]] List of normalizing flows to use for the unmixing function. Default: None. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. Methods ------- multi_env_log_prob(x, e, intervention_targets) -> Tensor Computes log probability of x in environment e. forward(x) -> Tensor Maps from the observed data x to the latent space v_hat. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Optional [ Tensor ] = None , fix_mechanisms : bool = False , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , flows : Optional [ list [ nf . flows . Flow ]] = None , q0 : Optional [ nf . distributions . BaseDistribution ] = None , K_cbn : int = 3 , net_hidden_dim_cbn : int = 128 , net_hidden_layers_cbn : int = 3 , ) -> None : self . latent_dim = latent_dim self . adjacency_matrix = adjacency_matrix self . intervention_targets_per_env = intervention_targets_per_env self . fix_mechanisms = fix_mechanisms self . fix_all_intervention_targets = fix_all_intervention_targets self . nonparametric_base_distr = nonparametric_base_distr self . K_cbn = K_cbn self . net_hidden_dim_cbn = net_hidden_dim_cbn self . net_hidden_layers_cbn = net_hidden_layers_cbn if q0 is None : if self . nonparametric_base_distr : q0 = NonparamMultiEnvCausalDistribution ( adjacency_matrix = adjacency_matrix , K = K_cbn , net_hidden_dim = net_hidden_dim_cbn , net_hidden_layers = net_hidden_layers_cbn , ) else : assert ( intervention_targets_per_env is not None ), \"intervention_targets_per_env must be provided for parametric base distribution\" q0 = ParamMultiEnvCausalDistribution ( adjacency_matrix = adjacency_matrix , intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , ) super () . __init__ ( q0 = q0 , flows = flows if flows is not None else []) def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : raise NotImplementedError def forward ( self , x : Tensor ) -> Tensor : raise NotImplementedError LinearCauCAEncoder Bases: CauCAEncoder Linear CauCA encoder for multi-environment data. Source code in model/encoder.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class LinearCauCAEncoder ( CauCAEncoder ): \"\"\" Linear CauCA encoder for multi-environment data. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Optional [ Tensor ] = None , fix_mechanisms : bool = True , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , nonparametric_base_distr = nonparametric_base_distr , ) self . unmixing = nn . Linear ( latent_dim , latent_dim , bias = False ) def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> tuple [ Tensor , dict [ str , Tensor ]]: v_hat = self ( x ) jacobian = torch . autograd . functional . jacobian ( self . unmixing , x [ 0 , :], create_graph = True ) log_q = torch . zeros ( len ( x ), dtype = x . dtype , device = x . device ) log_q += log ( abs ( det ( jacobian ))) determinant_terms = log_q prob_terms = self . q0 . multi_env_log_prob ( v_hat , e , intervention_targets ) log_q += prob_terms res = { \"log_prob\" : log_q , \"determinant_terms\" : determinant_terms , \"prob_terms\" : prob_terms , } return log_q , res def forward ( self , x : Tensor ) -> Tensor : latent = self . unmixing ( x ) return latent NaiveEncoder Bases: NonlinearCauCAEncoder Naive encoder for multi-environment data. This encoder does not assume any causal structure in the latent space. Equivalent to independent components analysis (ICA). Source code in model/encoder.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class NaiveEncoder ( NonlinearCauCAEncoder ): \"\"\" Naive encoder for multi-environment data. This encoder does not assume any causal structure in the latent space. Equivalent to independent components analysis (ICA). \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , K : int = 1 , intervention_targets_per_env : Optional [ Tensor ] = None , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , ) -> None : # overwrite the q0 from NonlinearICAEncoder q0 = NaiveMultiEnvCausalDistribution ( adjacency_matrix = adjacency_matrix , ) super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , K = K , intervention_targets_per_env = intervention_targets_per_env , net_hidden_dim = net_hidden_dim , net_hidden_layers = net_hidden_layers , q0 = q0 , ) NonlinearCauCAEncoder Bases: CauCAEncoder Nonlinear CauCA encoder for multi-environment data. Here the unmixing function is a normalizing flow. Parameters latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. K: int Number of normalizing flows to use for the unmixing function. Default: 1. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. net_hidden_dim: int Hidden dimension of the neural network used in the normalizing flows. Default: 128. net_hidden_layers: int Number of hidden layers in the neural network used in the normalizing flows. Default: 3. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. Source code in model/encoder.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 class NonlinearCauCAEncoder ( CauCAEncoder ): \"\"\" Nonlinear CauCA encoder for multi-environment data. Here the unmixing function is a normalizing flow. Parameters ---------- latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. K: int Number of normalizing flows to use for the unmixing function. Default: 1. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. net_hidden_dim: int Hidden dimension of the neural network used in the normalizing flows. Default: 128. net_hidden_layers: int Number of hidden layers in the neural network used in the normalizing flows. Default: 3. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , K : int = 1 , intervention_targets_per_env : Optional [ Tensor ] = None , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , fix_mechanisms : bool = True , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , q0 : Optional [ nf . distributions . BaseDistribution ] = None , K_cbn : int = 3 , net_hidden_dim_cbn : int = 128 , net_hidden_layers_cbn : int = 3 , ) -> None : self . K = K self . intervention_targets_per_env = intervention_targets_per_env self . net_hidden_dim = net_hidden_dim self . net_hidden_layers = net_hidden_layers flows = make_spline_flows ( K , latent_dim , net_hidden_dim , net_hidden_layers ) super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , nonparametric_base_distr = nonparametric_base_distr , flows = flows , q0 = q0 , K_cbn = K_cbn , net_hidden_dim_cbn = net_hidden_dim_cbn , net_hidden_layers_cbn = net_hidden_layers_cbn , ) def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> tuple [ Tensor , dict [ str , Tensor ]]: log_q = torch . zeros ( len ( x ), dtype = x . dtype , device = x . device ) z = x for i in range ( len ( self . flows ) - 1 , - 1 , - 1 ): z , log_det = self . flows [ i ] . inverse ( z ) log_q += log_det determinant_terms = log_q prob_terms = self . q0 . multi_env_log_prob ( z , e , intervention_targets ) log_q += prob_terms res = { \"log_prob\" : log_q , \"determinant_terms\" : determinant_terms , \"prob_terms\" : prob_terms , } return log_q , res def forward ( self , x : Tensor ) -> Tensor : return self . inverse ( x ) normalizing_flow distribution MultiEnvCausalDistribution Bases: BaseDistribution , ABC Base class for parametric multi-environment causal distributions. In typical normalizing flow architectures, the base distribution is a simple distribution such as a multivariate Gaussian. In our case, the base distribution has additional multi-environment causal structure. Hence, in the parametric case, this class learns the parameters of the causal mechanisms and noise distributions. The causal graph is assumed to be known. This is a subclass of BaseDistribution, which is a subclass of torch.nn.Module. Hence, this class can be used as a base distribution in a normalizing flow. Methods multi_env_log_prob(z, e, intervention_targets) -> Tensor Compute the log probability of the latent variables v in environment e, given the intervention targets. This is used as the main training objective. Source code in model/normalizing_flow/distribution.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MultiEnvCausalDistribution ( nf . distributions . BaseDistribution , ABC ): \"\"\" Base class for parametric multi-environment causal distributions. In typical normalizing flow architectures, the base distribution is a simple distribution such as a multivariate Gaussian. In our case, the base distribution has additional multi-environment causal structure. Hence, in the parametric case, this class learns the parameters of the causal mechanisms and noise distributions. The causal graph is assumed to be known. This is a subclass of BaseDistribution, which is a subclass of torch.nn.Module. Hence, this class can be used as a base distribution in a normalizing flow. Methods ------- multi_env_log_prob(z, e, intervention_targets) -> Tensor Compute the log probability of the latent variables v in environment e, given the intervention targets. This is used as the main training objective. \"\"\" def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : raise NotImplementedError NaiveMultiEnvCausalDistribution Bases: MultiEnvCausalDistribution Naive multi-environment causal distribution. This is a dummy-version of ParamMultiEnvCausalDistribution, where the causal mechanisms are assumed to be trivial (no connectioons between variables) and the noise distributions are assumed to be Gaussian and independent of the environment. This is equivalent to the independent component analysis (ICA) case. Source code in model/normalizing_flow/distribution.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 class NaiveMultiEnvCausalDistribution ( MultiEnvCausalDistribution ): \"\"\" Naive multi-environment causal distribution. This is a dummy-version of ParamMultiEnvCausalDistribution, where the causal mechanisms are assumed to be trivial (no connectioons between variables) and the noise distributions are assumed to be Gaussian and independent of the environment. This is equivalent to the independent component analysis (ICA) case. \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , ) -> None : super () . __init__ () self . adjacency_matrix = adjacency_matrix self . q0 = DiagGaussian ( adjacency_matrix . shape [ 0 ], trainable = True ) def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : return self . q0 . log_prob ( z ) ParamMultiEnvCausalDistribution Bases: MultiEnvCausalDistribution Parametric multi-environment causal distribution. This class learns the parameters of the causal mechanisms and noise distributions. The causal mechanisms are assumed to be linear, and the noise distributions are assumed to be Gaussian. In environments where a variable is intervened on, the connection to its parents is assumed to be cut off, and the noise distribution can be shifted relative to the observational environment (when the variable is not intervened on). Theoretically, we can fix some of the mechanisms involved w.l.o.g. and still achieve identifiability (see Appendix G2 of [1]). There are two ways to do this: 1. Fix all mechanisms that are intervened on. 2. Fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set. However, we do not have to fix any of the mechanisms and in practice, we find that this leads to better performance. Attributes adjacency_matrix: np.ndarray Adjacency matrix of the causal graph. fix_mechanisms: bool Whether to fix any of the mechanisms. Default: False. fix_all_intervention_targets: bool Whether to fix all mechanisms that are intervened on (option 1 above). If False, we fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set (option 2 above). Default: False. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. dag: nx.DiGraph Directed acyclic graph of the causal connections. coeff_values: nn.ParameterList List of lists of coefficients for the linear mechanisms. The outer list has length equal to the number of variables, and the inner list has length equal to the number of parents of the variable. The last element of the inner list is the variance parameter. I.e. coeff_values i are the linear weights of the parent variables of variable i, and coeff_values i is weight of the exogenous noise. noise_means: nn.ParameterList List of lists of means for the noise distributions. The outer list has length equal to the number of environments, and the inner list has length equal to the number of variables. noise_means e is the mean of the noise distribution for variable i in environment e. Note that not all of these parameters are used in the computation of the log probability. If a variable i is not intervened on in environment e, we use the observational noise distribution, i.e. noise_means 0 (e=0 is assumed to be the observational environment). noise_stds: nn.ParameterList Same as noise_means, but for the standard deviations of the noise distributions. coeff_values_requires_grad: list[list[bool]] Whether each coefficient is trainable. This is used to fix the coefficients of the mechanisms. noise_means_requires_grad: list[list[bool]] Whether each noise mean is trainable. This is used to fix the noise means of the mechanisms. noise_stds_requires_grad: list[list[bool]] Whether each noise standard deviation is trainable. This is used to fix the noise standard deviations References [1] https://arxiv.org/abs/2305.17225 Source code in model/normalizing_flow/distribution.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class ParamMultiEnvCausalDistribution ( MultiEnvCausalDistribution ): \"\"\" Parametric multi-environment causal distribution. This class learns the parameters of the causal mechanisms and noise distributions. The causal mechanisms are assumed to be linear, and the noise distributions are assumed to be Gaussian. In environments where a variable is intervened on, the connection to its parents is assumed to be cut off, and the noise distribution can be shifted relative to the observational environment (when the variable is not intervened on). Theoretically, we can fix some of the mechanisms involved w.l.o.g. and still achieve identifiability (see Appendix G2 of [1]). There are two ways to do this: 1. Fix all mechanisms that are intervened on. 2. Fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set. However, we do not have to fix any of the mechanisms and in practice, we find that this leads to better performance. Attributes ---------- adjacency_matrix: np.ndarray Adjacency matrix of the causal graph. fix_mechanisms: bool Whether to fix any of the mechanisms. Default: False. fix_all_intervention_targets: bool Whether to fix all mechanisms that are intervened on (option 1 above). If False, we fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set (option 2 above). Default: False. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. dag: nx.DiGraph Directed acyclic graph of the causal connections. coeff_values: nn.ParameterList List of lists of coefficients for the linear mechanisms. The outer list has length equal to the number of variables, and the inner list has length equal to the number of parents of the variable. The last element of the inner list is the variance parameter. I.e. coeff_values[i][:-1] are the linear weights of the parent variables of variable i, and coeff_values[i][-1] is weight of the exogenous noise. noise_means: nn.ParameterList List of lists of means for the noise distributions. The outer list has length equal to the number of environments, and the inner list has length equal to the number of variables. noise_means[e][i] is the mean of the noise distribution for variable i in environment e. Note that not all of these parameters are used in the computation of the log probability. If a variable i is not intervened on in environment e, we use the observational noise distribution, i.e. noise_means[0][i] (e=0 is assumed to be the observational environment). noise_stds: nn.ParameterList Same as noise_means, but for the standard deviations of the noise distributions. coeff_values_requires_grad: list[list[bool]] Whether each coefficient is trainable. This is used to fix the coefficients of the mechanisms. noise_means_requires_grad: list[list[bool]] Whether each noise mean is trainable. This is used to fix the noise means of the mechanisms. noise_stds_requires_grad: list[list[bool]] Whether each noise standard deviation is trainable. This is used to fix the noise standard deviations References ---------- [1] https://arxiv.org/abs/2305.17225 \"\"\" trainable = True def __init__ ( self , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , fix_mechanisms : bool = False , fix_all_intervention_targets : bool = False , ) -> None : super () . __init__ () self . adjacency_matrix = adjacency_matrix self . fix_mechanisms = fix_mechanisms self . fix_all_intervention_targets = fix_all_intervention_targets self . intervention_targets_per_env = intervention_targets_per_env self . dag = nx . DiGraph ( adjacency_matrix ) device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) coeff_values , coeff_values_requires_grad = self . _set_initial_coeffs ( self . dag , device ) noise_means , noise_means_requires_grad = self . _set_initial_noise_means ( self . dag , fix_mechanisms , intervention_targets_per_env , fix_all_intervention_targets , device , ) noise_stds , noise_stds_requires_grad = self . _set_initial_noise_stds ( self . dag , fix_mechanisms , intervention_targets_per_env , fix_all_intervention_targets , device , ) self . coeff_values = nn . ParameterList ( coeff_values ) self . noise_means = nn . ParameterList ( noise_means ) self . noise_stds = nn . ParameterList ( noise_stds ) self . coeff_values_requires_grad = coeff_values_requires_grad self . noise_means_requires_grad = noise_means_requires_grad self . noise_stds_requires_grad = noise_stds_requires_grad def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : log_p = torch . zeros ( len ( z ), dtype = z . dtype , device = z . device ) for env in e . unique (): env_mask = ( e == env ) . flatten () z_env = z [ env_mask , :] intervention_targets_env = intervention_targets [ env_mask , :] for i in range ( z . shape [ 1 ]): parents = list ( self . dag . predecessors ( i )) if len ( parents ) == 0 or intervention_targets_env [ 0 , i ] == 1 : parent_contribution = 0 else : coeffs_raw = self . coeff_values [ i ][: - 1 ] if isinstance ( coeffs_raw , nn . ParameterList ): coeffs_raw = torch . cat ([ c for c in coeffs_raw ]) parent_coeffs = coeffs_raw . to ( z . device ) parent_contribution = parent_coeffs . matmul ( z_env [:, parents ] . T ) noise_env_idx = int ( env ) if intervention_targets_env [ 0 , i ] == 1 else 0 var = self . noise_stds [ noise_env_idx ][ i ] ** 2 * torch . ones_like ( z_env [:, i ] ) noise_coeff = self . coeff_values [ i ][ - 1 ] . to ( z . device ) noise_contribution = noise_coeff * self . noise_means [ noise_env_idx ][ i ] var *= noise_coeff ** 2 log_p [ env_mask ] += torch . distributions . Normal ( parent_contribution + noise_contribution , var . sqrt () ) . log_prob ( z_env [:, i ]) return log_p @staticmethod def _set_initial_coeffs ( dag : nx . DiGraph , device : torch . device ) -> tuple [ list [ ParameterList ], list [ list [ bool ]]]: coeff_values = [] coeff_values_requires_grad = [] for i in range ( dag . number_of_nodes ()): coeff_values_i = [] coeff_values_requires_grad_i = [] num_parents = len ( list ( dag . predecessors ( i ))) for j in range ( num_parents ): random_val = Uniform ( - 1 , 1 ) . sample (( 1 ,)) val = random_val param = nn . Parameter ( val * torch . ones ( 1 ), requires_grad = True ) . to ( device ) coeff_values_i . append ( param ) coeff_values_requires_grad_i . append ( True ) const = torch . ones ( 1 , requires_grad = False ) . to ( device ) # variance param coeff_values_i . append ( const ) coeff_values_requires_grad_i . append ( False ) coeff_values . append ( nn . ParameterList ( coeff_values_i )) coeff_values_requires_grad . append ( coeff_values_requires_grad_i ) return coeff_values , coeff_values_requires_grad @staticmethod def _set_initial_noise_means ( dag : nx . DiGraph , fix_mechanisms : bool , intervention_targets_per_env : Tensor , fix_all_intervention_targets : bool , device : torch . device , ) -> tuple [ list [ ParameterList ], list [ list [ bool ]]]: noise_means = [] noise_means_requires_grad = [] num_envs = intervention_targets_per_env . shape [ 0 ] for e in range ( num_envs ): noise_means_e = [] noise_means_requires_grad_e = [] for i in range ( dag . number_of_nodes ()): is_shifted = intervention_targets_per_env [ e ][ i ] == 1 is_root = len ( list ( dag . predecessors ( i ))) == 0 if fix_all_intervention_targets : is_fixed = is_shifted else : is_fixed = ( is_shifted and not is_root ) or ( not is_shifted and is_root ) is_fixed = is_fixed and fix_mechanisms random_val = Uniform ( - 0.5 , 0.5 ) . sample (( 1 ,)) val = random_val param = ( nn . Parameter ( val * torch . ones ( 1 ), requires_grad = not is_fixed ) ) . to ( device ) noise_means_e . append ( param ) noise_means_requires_grad_e . append ( not is_fixed ) noise_means . append ( nn . ParameterList ( noise_means_e )) noise_means_requires_grad . append ( noise_means_requires_grad_e ) return noise_means , noise_means_requires_grad @staticmethod def _set_initial_noise_stds ( dag : nx . DiGraph , fix_mechanisms : bool , intervention_targets_per_env : Tensor , fix_all_intervention_targets : bool , device : torch . device , ) -> tuple [ list [ ParameterList ], list [ list [ bool ]]]: noise_stds = [] noise_stds_requires_grad = [] for e in range ( intervention_targets_per_env . shape [ 0 ]): noise_stds_e = [] noise_stds_requires_grad_e = [] for i in range ( dag . number_of_nodes ()): is_shifted = intervention_targets_per_env [ e ][ i ] == 1 is_root = len ( list ( dag . predecessors ( i ))) == 0 if fix_all_intervention_targets : is_fixed = is_shifted else : is_fixed = ( is_shifted and not is_root ) or ( not is_shifted and is_root ) is_fixed = is_fixed and fix_mechanisms random_val = Uniform ( 0.5 , 1.5 ) . sample (( 1 ,)) val = random_val param = ( nn . Parameter ( val * torch . ones ( 1 ), requires_grad = not is_fixed ) ) . to ( device ) noise_stds_e . append ( param ) noise_stds_requires_grad_e . append ( not is_fixed ) noise_stds . append ( nn . ParameterList ( noise_stds_e )) noise_stds_requires_grad . append ( noise_stds_requires_grad_e ) return noise_stds , noise_stds_requires_grad nonparametric_distribution MultiEnvBaseDistribution Bases: BaseDistribution Base distribution for nonparametric multi-environment causal distributions. This simple independent Gaussian distribution is used as the base distribution for the nonparametric multi-environment causal distribution. I.e. this distribution represents the exogenous noise in the SCM. Source code in model/normalizing_flow/nonparametric_distribution.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class MultiEnvBaseDistribution ( nf . distributions . BaseDistribution ): \"\"\" Base distribution for nonparametric multi-environment causal distributions. This simple independent Gaussian distribution is used as the base distribution for the nonparametric multi-environment causal distribution. I.e. this distribution represents the exogenous noise in the SCM. \"\"\" def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : gaussian_nll = gaussian_nll_loss ( x , torch . zeros_like ( x ), torch . ones_like ( x ), full = True , reduction = \"none\" ) mask = ~ intervention_targets . to ( bool ) log_p = - ( mask * gaussian_nll ) . sum ( dim = 1 ) return log_p NonparamMultiEnvCausalDistribution Bases: NormalizingFlow Nonarametric multi-environment causal distribution. A nonparametric causal distribution that uses a normalizing flow to parameterize the latent causal mechanisms. This causal distribution has two parts: 1. The latent SCM, which is parameterized by a normalizing flow. It represents the reduced form of the SCM, mapping independent (Gaussian) exogenous noise to the endogenous latent variables. The causal structure of the latent SCM is encoded through the topological order of the latent variables according to the adjacency matrix. 2. Fixed, simple base distributions for the mechanisms that are intervened on. Attributes adjacency_matrix : np.ndarray The adjacency matrix of the SCM. K : int The number of normalizing flow blocks to use for the reduced form of the SCM. net_hidden_dim : int The hidden dimension of the neural networks used in the normalizing flow blocks. net_hidden_layers : int The number of hidden layers of the neural networks used in the normalizing flow blocks. perm : torch.Tensor The permutation of the latent variables according to the topological order. Methods multi_env_log_prob(z, e, intervention_targets) -> torch.Tensor Compute the log probability of the given data. References [1] https://arxiv.org/abs/2305.17225 Source code in model/normalizing_flow/nonparametric_distribution.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class NonparamMultiEnvCausalDistribution ( nf . NormalizingFlow ): \"\"\" Nonarametric multi-environment causal distribution. A nonparametric causal distribution that uses a normalizing flow to parameterize the latent causal mechanisms. This causal distribution has two parts: 1. The latent SCM, which is parameterized by a normalizing flow. It represents the reduced form of the SCM, mapping independent (Gaussian) exogenous noise to the endogenous latent variables. The causal structure of the latent SCM is encoded through the topological order of the latent variables according to the adjacency matrix. 2. Fixed, simple base distributions for the mechanisms that are intervened on. Attributes ---------- adjacency_matrix : np.ndarray The adjacency matrix of the SCM. K : int The number of normalizing flow blocks to use for the reduced form of the SCM. net_hidden_dim : int The hidden dimension of the neural networks used in the normalizing flow blocks. net_hidden_layers : int The number of hidden layers of the neural networks used in the normalizing flow blocks. perm : torch.Tensor The permutation of the latent variables according to the topological order. Methods ------- multi_env_log_prob(z, e, intervention_targets) -> torch.Tensor Compute the log probability of the given data. References ---------- [1] https://arxiv.org/abs/2305.17225 \"\"\" trainable = True def __init__ ( self , adjacency_matrix : np . ndarray , K : int = 3 , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , ) -> None : self . adjacency_matrix = adjacency_matrix self . K = K self . net_hidden_dim = net_hidden_dim self . net_hidden_layers = net_hidden_layers latent_dim = adjacency_matrix . shape [ 0 ] # permutation according to topological order self . perm = torch . tensor ( list ( nx . topological_sort ( nx . DiGraph ( self . adjacency_matrix ))), dtype = torch . long , ) flows = make_spline_flows ( K , latent_dim , net_hidden_dim , net_hidden_layers , permutation = False ) q0 = MultiEnvBaseDistribution () super () . __init__ ( q0 = q0 , flows = flows ) def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : z = z [:, self . perm ] # permute inputs to be in topological order log_q , u = self . _determinant_terms ( intervention_targets , z ) prob_terms = self . q0 . multi_env_log_prob ( u , e , intervention_targets ) prob_terms_intervened = self . _prob_terms_intervened ( intervention_targets , z ) log_q += prob_terms + prob_terms_intervened return log_q def _determinant_terms ( self , intervention_targets : Tensor , z : Tensor ) -> tuple [ Tensor , Tensor ]: log_q = torch . zeros ( len ( z ), dtype = z . dtype , device = z . device ) u = z for i in range ( len ( self . flows ) - 1 , - 1 , - 1 ): u , log_det = self . flows [ i ] . inverse ( u ) log_q += log_det # remove determinant terms for intervened mechanisms jac_row = torch . autograd . functional . jvp ( self . inverse , z , v = intervention_targets , create_graph = True )[ 1 ] jac_diag_element = ( jac_row * intervention_targets ) . sum ( dim = 1 ) # mask zero elements not_intervened_mask = ~ intervention_targets . sum ( dim = 1 ) . to ( bool ) jac_diag_element [ not_intervened_mask ] = 1 log_q -= log ( abs ( jac_diag_element ) + 1e-8 ) return log_q , u def _prob_terms_intervened ( self , intervention_targets : Tensor , z : Tensor ) -> Tensor : \"\"\" Compute the probability terms for the intervened mechanisms. \"\"\" gaussian_nll = gaussian_nll_loss ( z , torch . zeros_like ( z ), torch . ones_like ( z ), full = True , reduction = \"none\" ) mask = intervention_targets . to ( bool ) prob_terms_intervention_targets = - ( mask * gaussian_nll ) . sum ( dim = 1 ) return prob_terms_intervention_targets","title":"API"},{"location":"references/#api-references","text":"","title":"API references"},{"location":"references/#data_generator.data_module","text":"","title":"data_module"},{"location":"references/#data_generator.data_module.MultiEnvDataModule","text":"Bases: LightningDataModule Data module for multi-environment data.","title":"MultiEnvDataModule"},{"location":"references/#data_generator.data_module.MultiEnvDataModule--attributes","text":"medgp: MultiEnvDGP Multi-environment data generating process. num_samples_per_env: int Number of samples per environment. batch_size: int Batch size. num_workers: int Number of workers for the data loaders. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on. log_dir: Optional[Path] Directory to save summary statistics and plots to. Default: None. intervention_target_misspec: bool Whether to misspecify the intervention targets. If true, the intervention targets are permuted. I.e. the model received the wrong intervention targets. Default: False. intervention_target_perm: Optional[list[int]] Permutation of the intervention targets. If None, a random permutation is used. Only used if intervention_target_misspec is True. Default: None.","title":"Attributes"},{"location":"references/#data_generator.data_module.MultiEnvDataModule--methods","text":"setup(stage=None) -> None Setup the data module. This is where the data is sampled. train_dataloader() -> DataLoader Return the training data loader. val_dataloader() -> DataLoader Return the validation data loader. test_dataloader() -> DataLoader Return the test data loader. Source code in data_generator/data_module.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class MultiEnvDataModule ( LightningDataModule ): \"\"\" Data module for multi-environment data. Attributes ---------- medgp: MultiEnvDGP Multi-environment data generating process. num_samples_per_env: int Number of samples per environment. batch_size: int Batch size. num_workers: int Number of workers for the data loaders. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on. log_dir: Optional[Path] Directory to save summary statistics and plots to. Default: None. intervention_target_misspec: bool Whether to misspecify the intervention targets. If true, the intervention targets are permuted. I.e. the model received the wrong intervention targets. Default: False. intervention_target_perm: Optional[list[int]] Permutation of the intervention targets. If None, a random permutation is used. Only used if intervention_target_misspec is True. Default: None. Methods ------- setup(stage=None) -> None Setup the data module. This is where the data is sampled. train_dataloader() -> DataLoader Return the training data loader. val_dataloader() -> DataLoader Return the validation data loader. test_dataloader() -> DataLoader Return the test data loader. \"\"\" def __init__ ( self , multi_env_dgp : MultiEnvDGP , num_samples_per_env : int , batch_size : int , num_workers : int , intervention_targets_per_env : Tensor , log_dir : Optional [ Path ] = None , intervention_target_misspec : bool = False , intervention_target_perm : Optional [ list [ int ]] = None , ) -> None : super () . __init__ () self . medgp = multi_env_dgp self . num_samples_per_env = num_samples_per_env self . batch_size = batch_size self . num_workers = num_workers self . intervention_targets_per_env = intervention_targets_per_env self . log_dir = log_dir self . intervention_target_misspec = intervention_target_misspec latent_dim = self . medgp . latent_scm . latent_dim assert ( intervention_target_perm is None or len ( intervention_target_perm ) == latent_dim ) self . intervention_target_perm = intervention_target_perm def setup ( self , stage : Optional [ str ] = None ) -> None : latent_dim = self . medgp . latent_scm . latent_dim num_envs = self . intervention_targets_per_env . shape [ 0 ] x , v , u , e , intervention_targets , log_prob = self . medgp . sample ( self . num_samples_per_env , intervention_targets_per_env = self . intervention_targets_per_env , ) if self . intervention_target_misspec : assert ( num_envs == latent_dim + 1 ), \"only works if num_envs == num_causal_variables + 1\" if self . intervention_target_perm is None : perm = random_perm ( latent_dim ) self . intervention_target_perm = perm else : perm = self . intervention_target_perm # remember where old targets were idx_mask_list = [] for i in range ( latent_dim ): idx_mask = intervention_targets [:, i ] == 1 idx_mask_list . append ( idx_mask ) intervention_targets [ idx_mask , i ] = 0 # permute targets for i in range ( latent_dim ): intervention_targets [ idx_mask_list [ i ], perm [ i ]] = 1 dataset = TensorDataset ( x , v , u , e , intervention_targets , log_prob ) train_size = int ( 0.8 * len ( dataset )) val_size = int ( 0.5 * ( len ( dataset ) - train_size )) test_size = len ( dataset ) - train_size - val_size ( self . train_dataset , self . val_dataset , self . test_dataset , ) = torch . utils . data . random_split ( dataset , [ train_size , val_size , test_size ]) if self . log_dir is not None : self . log_dir . mkdir ( parents = True , exist_ok = True ) summary_stats = summary_statistics ( x , v , e , intervention_targets ) for key , value in summary_stats . items (): value . to_csv ( self . log_dir / f \" { key } _summary_stats.csv\" ) plot_dag ( self . medgp . adjacency_matrix , self . log_dir ) try : with open ( self . log_dir / \"base_coeff_values.txt\" , \"w\" ) as f : f . write ( str ( self . medgp . latent_scm . base_coeff_values )) except AttributeError : pass # save mixing function coefficients self . medgp . mixing_function . save_coeffs ( self . log_dir ) def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers , ) def val_dataloader ( self ) -> DataLoader : val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers , ) return val_loader def test_dataloader ( self ) -> DataLoader : test_loader = DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers , ) return test_loader","title":"Methods"},{"location":"references/#data_generator.graph_sampler","text":"","title":"graph_sampler"},{"location":"references/#data_generator.graph_sampler.sample_random_dag","text":"Sample a random DAG with n_nodes nodes and edge_prob probability of an edge between two nodes. We ensure that there is at least one edge in the graph by rejecting graphs with no edges and resampling.","title":"sample_random_dag()"},{"location":"references/#data_generator.graph_sampler.sample_random_dag--parameters","text":"n_nodes: int Number of nodes in the graph. edge_prob: float Probability of an edge between two nodes.","title":"Parameters"},{"location":"references/#data_generator.graph_sampler.sample_random_dag--returns","text":"adjaceny_matrix: np.ndarray, shape (n_nodes, n_nodes) The adjacency matrix of the sampled DAG. Source code in data_generator/graph_sampler.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def sample_random_dag ( n_nodes : int , edge_prob : float ) -> np . ndarray : \"\"\" Sample a random DAG with n_nodes nodes and edge_prob probability of an edge between two nodes. We ensure that there is at least one edge in the graph by rejecting graphs with no edges and resampling. Parameters ---------- n_nodes: int Number of nodes in the graph. edge_prob: float Probability of an edge between two nodes. Returns ------- adjaceny_matrix: np.ndarray, shape (n_nodes, n_nodes) The adjacency matrix of the sampled DAG. \"\"\" while True : adjaceny_matrix = np . random . binomial ( 1 , edge_prob , size = ( n_nodes , n_nodes )) # put all lower triangular elements to zero adjaceny_matrix [ np . tril_indices ( n_nodes )] = 0 # make sure the graph has at least one edge if np . sum ( adjaceny_matrix ) > 0 : break return adjaceny_matrix","title":"Returns"},{"location":"references/#data_generator.mixing_function","text":"","title":"mixing_function"},{"location":"references/#data_generator.mixing_function.LinearMixing","text":"Bases: MixingFunction Linear mixing function. The coefficients are sampled from a uniform distribution.","title":"LinearMixing"},{"location":"references/#data_generator.mixing_function.LinearMixing--parameters","text":"latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. Source code in data_generator/mixing_function.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class LinearMixing ( MixingFunction ): \"\"\" Linear mixing function. The coefficients are sampled from a uniform distribution. Parameters ---------- latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. \"\"\" def __init__ ( self , latent_dim : int , observation_dim : int ) -> None : super () . __init__ ( latent_dim , observation_dim ) self . coeffs = torch . rand (( latent_dim , observation_dim )) def __call__ ( self , v : Tensor ) -> Tensor : return torch . matmul ( v , self . coeffs . to ( v . device )) def save_coeffs ( self , path : Path ) -> None : # save matrix coefficients torch . save ( self . coeffs , path / \"matrix.pt\" ) matrix_np = self . coeffs . numpy () # convert to Numpy array df = pd . DataFrame ( matrix_np ) # convert to a dataframe df . to_csv ( path / \"matrix.csv\" , index = False ) # save as csv","title":"Parameters"},{"location":"references/#data_generator.mixing_function.MixingFunction","text":"Bases: ABC Base class for mixing functions. The mixing function is the function that maps from the latent space to the observation space.","title":"MixingFunction"},{"location":"references/#data_generator.mixing_function.MixingFunction--parameters","text":"latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. Source code in data_generator/mixing_function.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class MixingFunction ( ABC ): \"\"\" Base class for mixing functions. The mixing function is the function that maps from the latent space to the observation space. Parameters ---------- latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. \"\"\" def __init__ ( self , latent_dim : int , observation_dim : int ) -> None : self . latent_dim = latent_dim self . observation_dim = observation_dim def __call__ ( self , v : Tensor ) -> Tensor : \"\"\" Apply the mixing function to the latent variables. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- x: Tensor, shape (num_samples, observation_dim) Observed variables. \"\"\" raise NotImplementedError () def save_coeffs ( self , path : Path ) -> None : \"\"\" Save the coefficients of the mixing function to disk. Parameters ---------- path: Path Path to save the coefficients to. \"\"\" raise NotImplementedError () def unmixing_jacobian ( self , v : Tensor ) -> Tensor : \"\"\" Compute the jacobian of the inverse mixing function using autograd and the inverse function theorem. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- unmixing_jacobian: Tensor, shape (num_samples, observation_dim, latent_dim) Jacobian of the inverse mixing function. References ---------- https://en.wikipedia.org/wiki/Inverse_function_theorem https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/7 \"\"\" func = self . __call__ inputs = v mixing_jacobian = torch . vmap ( torch . func . jacrev ( func ))( inputs ) unmixing_jacobian = torch . inverse ( mixing_jacobian ) return unmixing_jacobian","title":"Parameters"},{"location":"references/#data_generator.mixing_function.MixingFunction.__call__","text":"Apply the mixing function to the latent variables.","title":"__call__()"},{"location":"references/#data_generator.mixing_function.MixingFunction.__call__--parameters","text":"v: Tensor, shape (num_samples, latent_dim) Latent variables.","title":"Parameters"},{"location":"references/#data_generator.mixing_function.MixingFunction.__call__--returns","text":"x: Tensor, shape (num_samples, observation_dim) Observed variables. Source code in data_generator/mixing_function.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __call__ ( self , v : Tensor ) -> Tensor : \"\"\" Apply the mixing function to the latent variables. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- x: Tensor, shape (num_samples, observation_dim) Observed variables. \"\"\" raise NotImplementedError ()","title":"Returns"},{"location":"references/#data_generator.mixing_function.MixingFunction.save_coeffs","text":"Save the coefficients of the mixing function to disk.","title":"save_coeffs()"},{"location":"references/#data_generator.mixing_function.MixingFunction.save_coeffs--parameters","text":"path: Path Path to save the coefficients to. Source code in data_generator/mixing_function.py 45 46 47 48 49 50 51 52 53 54 def save_coeffs ( self , path : Path ) -> None : \"\"\" Save the coefficients of the mixing function to disk. Parameters ---------- path: Path Path to save the coefficients to. \"\"\" raise NotImplementedError ()","title":"Parameters"},{"location":"references/#data_generator.mixing_function.MixingFunction.unmixing_jacobian","text":"Compute the jacobian of the inverse mixing function using autograd and the inverse function theorem.","title":"unmixing_jacobian()"},{"location":"references/#data_generator.mixing_function.MixingFunction.unmixing_jacobian--parameters","text":"v: Tensor, shape (num_samples, latent_dim) Latent variables.","title":"Parameters"},{"location":"references/#data_generator.mixing_function.MixingFunction.unmixing_jacobian--returns","text":"unmixing_jacobian: Tensor, shape (num_samples, observation_dim, latent_dim) Jacobian of the inverse mixing function.","title":"Returns"},{"location":"references/#data_generator.mixing_function.MixingFunction.unmixing_jacobian--references","text":"https://en.wikipedia.org/wiki/Inverse_function_theorem https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/7 Source code in data_generator/mixing_function.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def unmixing_jacobian ( self , v : Tensor ) -> Tensor : \"\"\" Compute the jacobian of the inverse mixing function using autograd and the inverse function theorem. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Latent variables. Returns ------- unmixing_jacobian: Tensor, shape (num_samples, observation_dim, latent_dim) Jacobian of the inverse mixing function. References ---------- https://en.wikipedia.org/wiki/Inverse_function_theorem https://discuss.pytorch.org/t/computing-batch-jacobian-efficiently/80771/7 \"\"\" func = self . __call__ inputs = v mixing_jacobian = torch . vmap ( torch . func . jacrev ( func ))( inputs ) unmixing_jacobian = torch . inverse ( mixing_jacobian ) return unmixing_jacobian","title":"References"},{"location":"references/#data_generator.mixing_function.NonlinearMixing","text":"Bases: MixingFunction Nonlinear mixing function. The function is composed of a number of invertible matrices and leaky-tanh nonlinearities. I.e. we apply a random neural network to the latent variables.","title":"NonlinearMixing"},{"location":"references/#data_generator.mixing_function.NonlinearMixing--parameters","text":"latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. n_nonlinearities: int Number of layers (i.e. invertible maps and nonlinearities) in the mixing function. Default: 1. Source code in data_generator/mixing_function.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class NonlinearMixing ( MixingFunction ): \"\"\" Nonlinear mixing function. The function is composed of a number of invertible matrices and leaky-tanh nonlinearities. I.e. we apply a random neural network to the latent variables. Parameters ---------- latent_dim: int Dimension of the latent space. observation_dim: int Dimension of the observation space. n_nonlinearities: int Number of layers (i.e. invertible maps and nonlinearities) in the mixing function. Default: 1. \"\"\" def __init__ ( self , latent_dim : int , observation_dim : int , n_nonlinearities : int = 1 ) -> None : super () . __init__ ( latent_dim , observation_dim ) assert latent_dim == observation_dim self . coefs = torch . rand (( latent_dim , observation_dim )) self . n_nonlinearities = n_nonlinearities matrices = [] for i in range ( n_nonlinearities ): matrices . append ( sample_invertible_matrix ( observation_dim )) self . matrices = matrices nonlinearities = [] for i in range ( n_nonlinearities ): nonlinearities . append ( leaky_tanh ) self . nonlinearities = nonlinearities def __call__ ( self , v : Tensor ) -> Tensor : x = v for i in range ( self . n_nonlinearities ): mat = self . matrices [ i ] . to ( v . device ) nonlinearity = self . nonlinearities [ i ] x = nonlinearity ( torch . matmul ( x , mat )) return x def save_coeffs ( self , path : Path ) -> None : # save matrix coefficients for i in range ( self . n_nonlinearities ): torch . save ( self . matrices [ i ], path / f \"matrix_ { i } .pt\" ) matrix_np = self . matrices [ i ] . numpy () # convert to Numpy array df = pd . DataFrame ( matrix_np ) # convert to a dataframe df . to_csv ( path / f \"matrix_ { i } .csv\" , index = False ) # save as csv # save matrix determinants in one csv matrix_determinants = [] for i in range ( self . n_nonlinearities ): matrix_determinants . append ( torch . det ( self . matrices [ i ])) matrix_determinants_np = torch . stack ( matrix_determinants ) . numpy () df = pd . DataFrame ( matrix_determinants_np ) df . to_csv ( path / \"matrix_determinants.csv\" )","title":"Parameters"},{"location":"references/#data_generator.multi_env_gdp","text":"","title":"multi_env_gdp"},{"location":"references/#data_generator.multi_env_gdp.MultiEnvDGP","text":"Multi-environment data generating process (DGP). The DGP is defined by a latent structural causal model (SCM), a noise generator and a mixing function. This class is used to generate data from those three components. The latent SCM is a multi-environment SCM, i.e. it generates data for multiple environments which differ by interventions on some of the variables. The noise generator is also multi-environmental, i.e. it generates noise for multiple environments. The mixing function is a function that maps the latent variables to the observed variables. The mixing function is the same for all environments.","title":"MultiEnvDGP"},{"location":"references/#data_generator.multi_env_gdp.MultiEnvDGP--attributes","text":"mixing_function: MixingFunction Mixing function. latent_scm: MultiEnvLatentSCM Multi-environment latent SCM. noise_generator: MultiEnvNoise Multi-environment noise generator.","title":"Attributes"},{"location":"references/#data_generator.multi_env_gdp.MultiEnvDGP--methods","text":"sample(num_samples_per_env, intervention_targets_per_env) -> tuple[Tensor, ...] Sample from the DGP. Source code in data_generator/multi_env_gdp.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class MultiEnvDGP : \"\"\" Multi-environment data generating process (DGP). The DGP is defined by a latent structural causal model (SCM), a noise generator and a mixing function. This class is used to generate data from those three components. The latent SCM is a multi-environment SCM, i.e. it generates data for multiple environments which differ by interventions on some of the variables. The noise generator is also multi-environmental, i.e. it generates noise for multiple environments. The mixing function is a function that maps the latent variables to the observed variables. The mixing function is the same for all environments. Attributes ---------- mixing_function: MixingFunction Mixing function. latent_scm: MultiEnvLatentSCM Multi-environment latent SCM. noise_generator: MultiEnvNoise Multi-environment noise generator. Methods ------- sample(num_samples_per_env, intervention_targets_per_env) -> tuple[Tensor, ...] Sample from the DGP. \"\"\" def __init__ ( self , mixing_function : MixingFunction , latent_scm : MultiEnvLatentSCM , noise_generator : MultiEnvNoise , ) -> None : self . mixing_function = mixing_function self . latent_scm = latent_scm self . noise_generator = noise_generator self . adjacency_matrix = self . latent_scm . adjacency_matrix def sample ( self , num_samples_per_env : int , intervention_targets_per_env : Tensor , ) -> tuple [ Tensor , ... ]: \"\"\" Sample from the DGP. Parameters ---------- num_samples_per_env: int Number of samples to generate per environment. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. Returns ------- x: Tensor, shape (num_samples_per_env * num_envs, observation_dim) Samples of observed variables. v: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of latent variables. u: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of exogenous noise variables. e: Tensor, shape (num_samples_per_env * num_envs, 1) Environment indicator. intervention_targets: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Intervention targets. log_prob: Tensor, shape (num_samples_per_env * num_envs, 1) Ground-truth log probability of the samples. \"\"\" num_envs = intervention_targets_per_env . shape [ 0 ] shape = ( num_samples_per_env , num_envs , self . latent_scm . latent_dim , ) u = torch . zeros ( shape ) v = torch . zeros ( shape ) intervention_targets_out = torch . zeros ( shape ) e = torch . zeros (( num_samples_per_env , num_envs , 1 ), dtype = torch . long ) log_prob = torch . zeros (( num_samples_per_env , num_envs , 1 )) for env in range ( num_envs ): int_targets_env = intervention_targets_per_env [ env , :] noise_samples_env = self . noise_generator . sample ( env , size = num_samples_per_env ) noise_log_prob_env = self . noise_generator . log_prob ( noise_samples_env , env ) latent_samples_env = self . latent_scm . push_forward ( noise_samples_env , env ) log_det_scm = self . latent_scm . log_inverse_jacobian ( latent_samples_env , noise_samples_env , env ) intervention_targets_out [:, env , :] = int_targets_env u [:, env , :] = noise_samples_env v [:, env , :] = latent_samples_env e [:, env , :] = env log_prob [:, env , :] = ( log_det_scm + noise_log_prob_env . sum ( dim = 1 ) ) . unsqueeze ( 1 ) flattened_shape = ( num_samples_per_env * num_envs , self . latent_scm . latent_dim ) intervention_targets_out = intervention_targets_out . reshape ( flattened_shape ) u = u . reshape ( flattened_shape ) v = v . reshape ( flattened_shape ) e = e . reshape ( num_samples_per_env * num_envs , 1 ) log_prob = log_prob . reshape ( num_samples_per_env * num_envs , 1 ) x = self . mixing_function ( v ) unmixing_jacobian = self . mixing_function . unmixing_jacobian ( v ) log_det_unmixing_jacobian = torch . slogdet ( unmixing_jacobian ) . logabsdet . unsqueeze ( 1 ) log_prob += log_det_unmixing_jacobian return ( x , v , u , e , intervention_targets_out , log_prob , )","title":"Methods"},{"location":"references/#data_generator.multi_env_gdp.MultiEnvDGP.sample","text":"Sample from the DGP.","title":"sample()"},{"location":"references/#data_generator.multi_env_gdp.MultiEnvDGP.sample--parameters","text":"num_samples_per_env: int Number of samples to generate per environment. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments.","title":"Parameters"},{"location":"references/#data_generator.multi_env_gdp.MultiEnvDGP.sample--returns","text":"x: Tensor, shape (num_samples_per_env * num_envs, observation_dim) Samples of observed variables. v: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of latent variables. u: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of exogenous noise variables. e: Tensor, shape (num_samples_per_env * num_envs, 1) Environment indicator. intervention_targets: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Intervention targets. log_prob: Tensor, shape (num_samples_per_env * num_envs, 1) Ground-truth log probability of the samples. Source code in data_generator/multi_env_gdp.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def sample ( self , num_samples_per_env : int , intervention_targets_per_env : Tensor , ) -> tuple [ Tensor , ... ]: \"\"\" Sample from the DGP. Parameters ---------- num_samples_per_env: int Number of samples to generate per environment. intervention_targets_per_env: Tensor, shape (num_envs, num_causal_variables) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. Returns ------- x: Tensor, shape (num_samples_per_env * num_envs, observation_dim) Samples of observed variables. v: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of latent variables. u: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Samples of exogenous noise variables. e: Tensor, shape (num_samples_per_env * num_envs, 1) Environment indicator. intervention_targets: Tensor, shape (num_samples_per_env * num_envs, latent_dim) Intervention targets. log_prob: Tensor, shape (num_samples_per_env * num_envs, 1) Ground-truth log probability of the samples. \"\"\" num_envs = intervention_targets_per_env . shape [ 0 ] shape = ( num_samples_per_env , num_envs , self . latent_scm . latent_dim , ) u = torch . zeros ( shape ) v = torch . zeros ( shape ) intervention_targets_out = torch . zeros ( shape ) e = torch . zeros (( num_samples_per_env , num_envs , 1 ), dtype = torch . long ) log_prob = torch . zeros (( num_samples_per_env , num_envs , 1 )) for env in range ( num_envs ): int_targets_env = intervention_targets_per_env [ env , :] noise_samples_env = self . noise_generator . sample ( env , size = num_samples_per_env ) noise_log_prob_env = self . noise_generator . log_prob ( noise_samples_env , env ) latent_samples_env = self . latent_scm . push_forward ( noise_samples_env , env ) log_det_scm = self . latent_scm . log_inverse_jacobian ( latent_samples_env , noise_samples_env , env ) intervention_targets_out [:, env , :] = int_targets_env u [:, env , :] = noise_samples_env v [:, env , :] = latent_samples_env e [:, env , :] = env log_prob [:, env , :] = ( log_det_scm + noise_log_prob_env . sum ( dim = 1 ) ) . unsqueeze ( 1 ) flattened_shape = ( num_samples_per_env * num_envs , self . latent_scm . latent_dim ) intervention_targets_out = intervention_targets_out . reshape ( flattened_shape ) u = u . reshape ( flattened_shape ) v = v . reshape ( flattened_shape ) e = e . reshape ( num_samples_per_env * num_envs , 1 ) log_prob = log_prob . reshape ( num_samples_per_env * num_envs , 1 ) x = self . mixing_function ( v ) unmixing_jacobian = self . mixing_function . unmixing_jacobian ( v ) log_det_unmixing_jacobian = torch . slogdet ( unmixing_jacobian ) . logabsdet . unsqueeze ( 1 ) log_prob += log_det_unmixing_jacobian return ( x , v , u , e , intervention_targets_out , log_prob , )","title":"Returns"},{"location":"references/#data_generator.multi_env_gdp.make_multi_env_dgp","text":"Create a multi-environment data generating process (DGP).","title":"make_multi_env_dgp()"},{"location":"references/#data_generator.multi_env_gdp.make_multi_env_dgp--parameters","text":"latent_dim: int Dimension of the latent variables. observation_dim: int Dimension of the observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent SCM. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. shift_noise: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. noise_shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". mixing: str Mixing function. Options: \"linear\" or \"nonlinear\". Default: \"nonlinear\". scm: str Latent SCM. Options: \"linear\" or \"location-scale\". Default: \"linear\". n_nonlinearities: int Number of nonlinearities in the nonlinear mixing function. Default: 1. scm_coeffs_low: float Lower bound of the SCM coefficients in linear SCMs. Default: -1. scm_coeffs_high: float Upper bound of the SCM coefficients in linear SCMs. Default: 1. coeffs_min_abs_value: float Minimum absolute value of the SCM coefficients in linear SCMs. If None, no minimum absolute value is enforced. Default: None. edge_prob: float Probability of an edge in the adjacency matrix if no adjacency matrix is given. Default: None. snr: float Signal-to-noise ratio of the location-scale SCM. Default: 1.0.","title":"Parameters"},{"location":"references/#data_generator.multi_env_gdp.make_multi_env_dgp--returns","text":"medgp: MultiEnvDGP Multi-environment data generating process. Source code in data_generator/multi_env_gdp.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def make_multi_env_dgp ( latent_dim : int , observation_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , shift_noise : bool = True , noise_shift_type : str = \"mean\" , mixing : str = \"nonlinear\" , scm : str = \"linear\" , n_nonlinearities : int = 1 , scm_coeffs_low : float = - 1 , scm_coeffs_high : float = 1 , coeffs_min_abs_value : float = None , edge_prob : float = None , snr : float = 1.0 , ) -> MultiEnvDGP : \"\"\" Create a multi-environment data generating process (DGP). Parameters ---------- latent_dim: int Dimension of the latent variables. observation_dim: int Dimension of the observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent SCM. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. shift_noise: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. noise_shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". mixing: str Mixing function. Options: \"linear\" or \"nonlinear\". Default: \"nonlinear\". scm: str Latent SCM. Options: \"linear\" or \"location-scale\". Default: \"linear\". n_nonlinearities: int Number of nonlinearities in the nonlinear mixing function. Default: 1. scm_coeffs_low: float Lower bound of the SCM coefficients in linear SCMs. Default: -1. scm_coeffs_high: float Upper bound of the SCM coefficients in linear SCMs. Default: 1. coeffs_min_abs_value: float Minimum absolute value of the SCM coefficients in linear SCMs. If None, no minimum absolute value is enforced. Default: None. edge_prob: float Probability of an edge in the adjacency matrix if no adjacency matrix is given. Default: None. snr: float Signal-to-noise ratio of the location-scale SCM. Default: 1.0. Returns ------- medgp: MultiEnvDGP Multi-environment data generating process. \"\"\" if mixing == \"linear\" : mixing_function = LinearMixing ( latent_dim = latent_dim , observation_dim = observation_dim ) elif mixing == \"nonlinear\" : mixing_function = NonlinearMixing ( latent_dim = latent_dim , observation_dim = observation_dim , n_nonlinearities = n_nonlinearities , ) else : raise ValueError ( f \"Unknown mixing function { mixing } \" ) # if adjacency_matrix is not given as numpy array, sample a random one if not isinstance ( adjacency_matrix , np . ndarray ): assert ( edge_prob is not None ), \"edge_prob must be given if no adjacency_matrix is given\" adjacency_matrix = sample_random_dag ( latent_dim , edge_prob ) adjacency_matrix = adjacency_matrix if scm == \"linear\" : latent_scm = LinearSCM ( adjacency_matrix = adjacency_matrix , latent_dim = latent_dim , intervention_targets_per_env = intervention_targets_per_env , coeffs_low = scm_coeffs_low , coeffs_high = scm_coeffs_high , coeffs_min_abs_value = coeffs_min_abs_value , ) elif scm == \"location-scale\" : latent_scm = LocationScaleSCM ( adjacency_matrix = adjacency_matrix , latent_dim = latent_dim , intervention_targets_per_env = intervention_targets_per_env , snr = snr , ) else : raise ValueError ( f \"Unknown SCM { scm } \" ) noise_generator = GaussianNoise ( latent_dim = latent_dim , intervention_targets_per_env = intervention_targets_per_env , shift = shift_noise , shift_type = noise_shift_type , ) medgp = MultiEnvDGP ( latent_scm = latent_scm , noise_generator = noise_generator , mixing_function = mixing_function , ) return medgp","title":"Returns"},{"location":"references/#data_generator.noise_generator","text":"","title":"noise_generator"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise","text":"Bases: ABC Base class for multi-environment noise generators.","title":"MultiEnvNoise"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise--attributes","text":"latent_dim: int Latent dimension. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. mean: float Mean of the noise distribution. If shift is True and shift_type is \"mean\", the mean of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 0.0. std: float Standard deviation of the noise distribution. If shift is True and shift_type is \"std\", the standard deviation of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 1.0. shift: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\".","title":"Attributes"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise--methods","text":"sample(e, size=1) -> Tensor Sample from the noise distribution for a given environment. Source code in data_generator/noise_generator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class MultiEnvNoise ( ABC ): \"\"\" Base class for multi-environment noise generators. Attributes ---------- latent_dim: int Latent dimension. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. mean: float Mean of the noise distribution. If shift is True and shift_type is \"mean\", the mean of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 0.0. std: float Standard deviation of the noise distribution. If shift is True and shift_type is \"std\", the standard deviation of the noise distribution is shifted up or down depending on whether the mechanism is intervened on or not. Default: 1.0. shift: bool Whether to shift the noise distribution for variables that are intervened on. Default: False. shift_type: str Whether to shift the mean or standard deviation of the noise distribution for variables that are intervened on. Options: \"mean\" or \"std\". Default: \"mean\". Methods ------- sample(e, size=1) -> Tensor Sample from the noise distribution for a given environment. \"\"\" def __init__ ( self , latent_dim : int , intervention_targets_per_env : Tensor , mean : float = 0.0 , std : float = 1.0 , shift : bool = False , shift_type : str = \"mean\" , ) -> None : self . latent_dim = latent_dim self . intervention_targets = intervention_targets_per_env self . mean = mean self . std = std self . shift = shift assert shift_type in [ \"mean\" , \"std\" ], f \"Invalid shift type: { shift_type } \" self . shift_type = shift_type def sample ( self , e : int , size : int = 1 ) -> Tensor : \"\"\" Sample from the noise distribution for a given environment. Parameters ---------- e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. size: int Number of samples to generate. Default: 1. Returns ------- Tensor, shape (size, latent_dim) Samples from the noise distribution. \"\"\" raise NotImplementedError () def log_prob ( self , u : Tensor , e : int ) -> Tensor : \"\"\" Compute the log probability of u under the noise distribution for a given environment. We assume that all samples come from the same environment. Parameters ---------- u: Tensor, shape (size, latent_dim) Samples from the noise distribution. e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. Returns ------- log_prob: Tensor, shape (size, latent_dim) Log probability of u. \"\"\" raise NotImplementedError ()","title":"Methods"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise.log_prob","text":"Compute the log probability of u under the noise distribution for a given environment. We assume that all samples come from the same environment.","title":"log_prob()"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise.log_prob--parameters","text":"u: Tensor, shape (size, latent_dim) Samples from the noise distribution. e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable.","title":"Parameters"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise.log_prob--returns","text":"log_prob: Tensor, shape (size, latent_dim) Log probability of u. Source code in data_generator/noise_generator.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def log_prob ( self , u : Tensor , e : int ) -> Tensor : \"\"\" Compute the log probability of u under the noise distribution for a given environment. We assume that all samples come from the same environment. Parameters ---------- u: Tensor, shape (size, latent_dim) Samples from the noise distribution. e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. Returns ------- log_prob: Tensor, shape (size, latent_dim) Log probability of u. \"\"\" raise NotImplementedError ()","title":"Returns"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise.sample","text":"Sample from the noise distribution for a given environment.","title":"sample()"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise.sample--parameters","text":"e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. size: int Number of samples to generate. Default: 1.","title":"Parameters"},{"location":"references/#data_generator.noise_generator.MultiEnvNoise.sample--returns","text":"Tensor, shape (size, latent_dim) Samples from the noise distribution. Source code in data_generator/noise_generator.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def sample ( self , e : int , size : int = 1 ) -> Tensor : \"\"\" Sample from the noise distribution for a given environment. Parameters ---------- e: int Environment index. Must be in {0, ..., num_envs-1}. The number of environments is implicitly defined by the intervention_targets_per_env variable. size: int Number of samples to generate. Default: 1. Returns ------- Tensor, shape (size, latent_dim) Samples from the noise distribution. \"\"\" raise NotImplementedError ()","title":"Returns"},{"location":"references/#data_generator.scm","text":"","title":"scm"},{"location":"references/#data_generator.scm.LinearSCM","text":"Bases: MultiEnvLatentSCM Multi-environment latent SCM, where all causal mechanisms are linear. The coefficients of the linear causal mechanisms are sampled from a uniform distribution. Inherits all attributes and methods from MultiEnvLatentSCM.","title":"LinearSCM"},{"location":"references/#data_generator.scm.LinearSCM--additional-attributes","text":"coeffs_low : float Lower bound for the coefficients of the linear causal mechanisms. Default: -1.0. coeffs_high : float Upper bound for the coefficients of the linear causal mechanisms. Default: 1.0. coeffs_min_abs_value : Optional[float] Minimum absolute value for the coefficients of the linear causal mechanisms. If None, no minimum absolute value is enforced. Default: None.","title":"Additional attributes"},{"location":"references/#data_generator.scm.LinearSCM--additional-methods","text":"setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Source code in data_generator/scm.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 class LinearSCM ( MultiEnvLatentSCM ): \"\"\" Multi-environment latent SCM, where all causal mechanisms are linear. The coefficients of the linear causal mechanisms are sampled from a uniform distribution. Inherits all attributes and methods from MultiEnvLatentSCM. Additional attributes --------------------- coeffs_low : float Lower bound for the coefficients of the linear causal mechanisms. Default: -1.0. coeffs_high : float Upper bound for the coefficients of the linear causal mechanisms. Default: 1.0. coeffs_min_abs_value : Optional[float] Minimum absolute value for the coefficients of the linear causal mechanisms. If None, no minimum absolute value is enforced. Default: None. Additional methods ------------------ setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , coeffs_low : float = - 1.0 , coeffs_high : float = 1.0 , coeffs_min_abs_value : Optional [ float ] = None , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) coeffs_low: float coeffs_high: float coeffs_min_abs_value: Optional[float] \"\"\" super () . __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , ) self . coeffs_low = coeffs_low self . coeffs_high = coeffs_high self . coeffs_min_abs_value = coeffs_min_abs_value base_functions = [] base_inverse_jac = [] base_coeff_values = [] for index in range ( self . latent_dim ): parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = sample_coeffs ( self . coeffs_low , self . coeffs_high , ( len ( parents ) + 1 ,), min_abs_value = self . coeffs_min_abs_value , ) coeffs [ - 1 ] = 1 # set the noise coefficient to 1 base_functions . append ( partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs ) ) base_inverse_jac . append ( partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) ) base_coeff_values . append ( coeffs ) self . base_functions = base_functions self . base_inverse_jac = base_inverse_jac self . base_coeff_values = base_coeff_values self . functions_per_env , self . inverse_jac_per_env = self . setup_functions_per_env ( intervention_targets_per_env ) def setup_functions_per_env ( self , intervention_targets_per_env : Tensor ) -> tuple [ dict [ int , callable ], dict [ int , callable ]]: \"\"\" Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Parameters ---------- intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets for each environment. Returns ------- functions_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. \"\"\" functions_per_env = {} inverse_jac_per_env = {} num_envs = intervention_targets_per_env . shape [ 0 ] for env in range ( num_envs ): functions_env = {} inverse_jac_env = {} for index in self . topological_order : if intervention_targets_per_env [ env ][ index ] == 1 : parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = torch . zeros (( len ( parents ) + 1 ,)) # cut edges from parents coeffs [ - 1 ] = 1.0 # still use noise f = partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs , ) inverse_jac = partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) else : f = self . base_functions [ index ] inverse_jac = self . base_inverse_jac [ index ] functions_env [ index ] = f inverse_jac_env [ index ] = inverse_jac functions_per_env [ env ] = functions_env inverse_jac_per_env [ env ] = inverse_jac_env return functions_per_env , inverse_jac_per_env","title":"Additional methods"},{"location":"references/#data_generator.scm.LinearSCM.__init__","text":"","title":"__init__()"},{"location":"references/#data_generator.scm.LinearSCM.__init__--parameters","text":"adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) coeffs_low: float coeffs_high: float coeffs_min_abs_value: Optional[float] Source code in data_generator/scm.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , coeffs_low : float = - 1.0 , coeffs_high : float = 1.0 , coeffs_min_abs_value : Optional [ float ] = None , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) coeffs_low: float coeffs_high: float coeffs_min_abs_value: Optional[float] \"\"\" super () . __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , ) self . coeffs_low = coeffs_low self . coeffs_high = coeffs_high self . coeffs_min_abs_value = coeffs_min_abs_value base_functions = [] base_inverse_jac = [] base_coeff_values = [] for index in range ( self . latent_dim ): parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = sample_coeffs ( self . coeffs_low , self . coeffs_high , ( len ( parents ) + 1 ,), min_abs_value = self . coeffs_min_abs_value , ) coeffs [ - 1 ] = 1 # set the noise coefficient to 1 base_functions . append ( partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs ) ) base_inverse_jac . append ( partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) ) base_coeff_values . append ( coeffs ) self . base_functions = base_functions self . base_inverse_jac = base_inverse_jac self . base_coeff_values = base_coeff_values self . functions_per_env , self . inverse_jac_per_env = self . setup_functions_per_env ( intervention_targets_per_env )","title":"Parameters"},{"location":"references/#data_generator.scm.LinearSCM.setup_functions_per_env","text":"Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined.","title":"setup_functions_per_env()"},{"location":"references/#data_generator.scm.LinearSCM.setup_functions_per_env--parameters","text":"intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets for each environment.","title":"Parameters"},{"location":"references/#data_generator.scm.LinearSCM.setup_functions_per_env--returns","text":"functions_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. Source code in data_generator/scm.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def setup_functions_per_env ( self , intervention_targets_per_env : Tensor ) -> tuple [ dict [ int , callable ], dict [ int , callable ]]: \"\"\" Set up the functions_per_env and inverse_jac_per_env attributes. This is where the linear causal mechanisms are defined. Parameters ---------- intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets for each environment. Returns ------- functions_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env: dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. \"\"\" functions_per_env = {} inverse_jac_per_env = {} num_envs = intervention_targets_per_env . shape [ 0 ] for env in range ( num_envs ): functions_env = {} inverse_jac_env = {} for index in self . topological_order : if intervention_targets_per_env [ env ][ index ] == 1 : parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) coeffs = torch . zeros (( len ( parents ) + 1 ,)) # cut edges from parents coeffs [ - 1 ] = 1.0 # still use noise f = partial ( linear_base_func , index = index , parents = parents , coeffs = coeffs , ) inverse_jac = partial ( linear_inverse_jacobian , index = index , parents = parents , coeffs = coeffs , ) else : f = self . base_functions [ index ] inverse_jac = self . base_inverse_jac [ index ] functions_env [ index ] = f inverse_jac_env [ index ] = inverse_jac functions_per_env [ env ] = functions_env inverse_jac_per_env [ env ] = inverse_jac_env return functions_per_env , inverse_jac_per_env","title":"Returns"},{"location":"references/#data_generator.scm.LocationScaleSCM","text":"Bases: MultiEnvLatentSCM Multi-environment latent SCM, where all causal mechanisms are location-scale functions [1] of the form v_i = snr * f_loc(pa_i) + f_scale(u_i), where f_loc and f_scale are random nonlinear functions, pa_i are the parents of v_i, and u_i is the exogenous noise variable corresponding to v_i. snr is the signal-to-noise ratio. Inherits all attributes and methods from MultiEnvLatentSCM.","title":"LocationScaleSCM"},{"location":"references/#data_generator.scm.LocationScaleSCM--additional-attributes","text":"n_nonlinearities : int Number of nonlinearities in the location-scale functions. Default: 3. snr : float Signal-to-noise ratio. Default: 1.0. base_functions : list[callable] List of base functions that implement the location-scale functions for each latent variable in the unintervened (observational) environment. base_inverse_jac : list[callable] List of base functions that implement the log of the inverse Jacobian of the location-scale functions for each latent variable in the unintervened (observational) environment.","title":"Additional attributes"},{"location":"references/#data_generator.scm.LocationScaleSCM--additional-methods","text":"setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the causal mechanisms based on the location-scale functions are defined.","title":"Additional methods"},{"location":"references/#data_generator.scm.LocationScaleSCM--references","text":"[1] https://en.wikipedia.org/wiki/Location%E2%80%93scale_family Source code in data_generator/scm.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 class LocationScaleSCM ( MultiEnvLatentSCM ): \"\"\" Multi-environment latent SCM, where all causal mechanisms are location-scale functions [1] of the form v_i = snr * f_loc(pa_i) + f_scale(u_i), where f_loc and f_scale are random nonlinear functions, pa_i are the parents of v_i, and u_i is the exogenous noise variable corresponding to v_i. snr is the signal-to-noise ratio. Inherits all attributes and methods from MultiEnvLatentSCM. Additional attributes --------------------- n_nonlinearities : int Number of nonlinearities in the location-scale functions. Default: 3. snr : float Signal-to-noise ratio. Default: 1.0. base_functions : list[callable] List of base functions that implement the location-scale functions for each latent variable in the unintervened (observational) environment. base_inverse_jac : list[callable] List of base functions that implement the log of the inverse Jacobian of the location-scale functions for each latent variable in the unintervened (observational) environment. Additional methods ------------------ setup_functions_per_env(intervention_targets_per_env: Tensor) -> tuple[dict[int, callable], dict[int, callable]] Set up the functions_per_env and inverse_jac_per_env attributes. This is where the causal mechanisms based on the location-scale functions are defined. References ---------- [1] https://en.wikipedia.org/wiki/Location%E2%80%93scale_family \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , n_nonlinearities : int = 3 , snr : float = 1.0 , ) -> None : super () . __init__ ( adjacency_matrix , latent_dim , intervention_targets_per_env , ) self . n_nonlinearities = n_nonlinearities self . snr = snr base_functions = [] base_inverse_jac = [] for index in range ( self . latent_dim ): parents = torch . tensor ( list ( self . dag . predecessors ( index )), dtype = torch . int64 ) f , inverse_jac = make_location_scale_function ( index , parents , n_nonlinearities , snr ) base_functions . append ( f ) base_inverse_jac . append ( inverse_jac ) self . base_functions = base_functions self . base_inverse_jac = base_inverse_jac self . functions_per_env , self . inverse_jac_per_env = self . setup_functions_per_env ( intervention_targets_per_env ) def setup_functions_per_env ( self , intervention_targets_per_env : Tensor ) -> tuple [ dict [ int , callable ], dict [ int , callable ]]: functions_per_env = {} inverse_jac_per_env = {} num_envs = intervention_targets_per_env . shape [ 0 ] for env in range ( num_envs ): functions_env = {} inverse_jac_env = {} for index in self . topological_order : if intervention_targets_per_env [ env ][ index ] == 1 : parents = [] f , inverse_jac = make_location_scale_function ( index , parents , self . n_nonlinearities , self . snr ) else : f = self . base_functions [ index ] inverse_jac = self . base_inverse_jac [ index ] functions_env [ index ] = f inverse_jac_env [ index ] = inverse_jac functions_per_env [ env ] = functions_env inverse_jac_per_env [ env ] = inverse_jac_env return functions_per_env , inverse_jac_per_env","title":"References"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM","text":"Bases: ABC Base class for multi-environment latent SCM. In environments where a variable is intervened on, the dependencies of the variable are cut. Note that this class only implements the causal mechanisms. The exogenous noise variables, which mayb also shift under interventions, are implemented in the noise generator.","title":"MultiEnvLatentSCM"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM--attributes","text":"adjacency_matrix : np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the SCM. latent_dim : int Dimension of the latent space. intervention_targets_per_env : Tensor, shape (num_envs, latent_dim) Binary tensor indicating which variables are intervened on in each environment. dag : nx.DiGraph Directed acyclic graph representing the causal structure. topological_order : list[int] Topological order of the causal graph. functions_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env env is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index.","title":"Attributes"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM--methods","text":"push_forward(u: Tensor, env: int) -> Tensor Push forward the latent variable u through the SCM in environment env. log_inverse_jacobian(v: Tensor, u: Tensor, env: int) -> Tensor Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Source code in data_generator/scm.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class MultiEnvLatentSCM ( ABC ): \"\"\" Base class for multi-environment latent SCM. In environments where a variable is intervened on, the dependencies of the variable are cut. Note that this class only implements the causal mechanisms. The exogenous noise variables, which mayb also shift under interventions, are implemented in the noise generator. Attributes ---------- adjacency_matrix : np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the SCM. latent_dim : int Dimension of the latent space. intervention_targets_per_env : Tensor, shape (num_envs, latent_dim) Binary tensor indicating which variables are intervened on in each environment. dag : nx.DiGraph Directed acyclic graph representing the causal structure. topological_order : list[int] Topological order of the causal graph. functions_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the causal mechanism. I.e. functions_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size, latent_dim) that contains the result of applying the causal mechanism the parents of index. inverse_jac_per_env : dict[int, dict[int, callable]] Dictionary mapping environment indices to dictionaries mapping latent variable indices to functions that implement the log of the inverse Jacobian of the causal mechanism. I.e. inverse_jac_per_env[env][index] is a function that takes two arguments, v and u, and returns a Tensor of shape (batch_size,) that contains the log of the inverse Jacobian of the causal mechanism applied to the parents of index. Methods ------- push_forward(u: Tensor, env: int) -> Tensor Push forward the latent variable u through the SCM in environment env. log_inverse_jacobian(v: Tensor, u: Tensor, env: int) -> Tensor Compute the log of the inverse Jacobian of the SCM in environment env at v and u. \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) \"\"\" self . adjacency_matrix = adjacency_matrix self . latent_dim = latent_dim self . intervention_targets_per_env = intervention_targets_per_env assert adjacency_matrix . shape [ 0 ] == adjacency_matrix . shape [ 1 ] == latent_dim self . dag = nx . DiGraph ( adjacency_matrix ) self . topological_order = list ( nx . topological_sort ( self . dag )) self . functions_per_env = None self . inverse_jac_per_env = None def push_forward ( self , u : Tensor , env : int ) -> Tensor : \"\"\" Push forward the latent variable u through the SCM in environment env. Parameters ---------- u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. \"\"\" v = torch . nan * torch . zeros_like ( u ) for index in self . topological_order : f = self . functions_per_env [ env ][ index ] v [:, index ] = f ( v , u ) return v def log_inverse_jacobian ( self , v : Tensor , u : Tensor , env : int ) -> Tensor : \"\"\" Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- log_inv_jac: Tensor, shape (num_samples,) Log of the inverse Jacobian of the SCM at v and u. \"\"\" log_inv_jac = 0.0 for index in self . topological_order : log_inv_jac += torch . log ( self . inverse_jac_per_env [ env ][ index ]( v , u )) return log_inv_jac","title":"Methods"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.__init__","text":"","title":"__init__()"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.__init__--parameters","text":"adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Source code in data_generator/scm.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , adjacency_matrix : np . ndarray , latent_dim : int , intervention_targets_per_env : Tensor , ) -> None : \"\"\" Parameters ---------- adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) latent_dim: int intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) \"\"\" self . adjacency_matrix = adjacency_matrix self . latent_dim = latent_dim self . intervention_targets_per_env = intervention_targets_per_env assert adjacency_matrix . shape [ 0 ] == adjacency_matrix . shape [ 1 ] == latent_dim self . dag = nx . DiGraph ( adjacency_matrix ) self . topological_order = list ( nx . topological_sort ( self . dag )) self . functions_per_env = None self . inverse_jac_per_env = None","title":"Parameters"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.log_inverse_jacobian","text":"Compute the log of the inverse Jacobian of the SCM in environment env at v and u.","title":"log_inverse_jacobian()"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.log_inverse_jacobian--parameters","text":"v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index.","title":"Parameters"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.log_inverse_jacobian--returns","text":"log_inv_jac: Tensor, shape (num_samples,) Log of the inverse Jacobian of the SCM at v and u. Source code in data_generator/scm.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def log_inverse_jacobian ( self , v : Tensor , u : Tensor , env : int ) -> Tensor : \"\"\" Compute the log of the inverse Jacobian of the SCM in environment env at v and u. Parameters ---------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- log_inv_jac: Tensor, shape (num_samples,) Log of the inverse Jacobian of the SCM at v and u. \"\"\" log_inv_jac = 0.0 for index in self . topological_order : log_inv_jac += torch . log ( self . inverse_jac_per_env [ env ][ index ]( v , u )) return log_inv_jac","title":"Returns"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.push_forward","text":"Push forward the latent variable u through the SCM in environment env.","title":"push_forward()"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.push_forward--parameters","text":"u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index.","title":"Parameters"},{"location":"references/#data_generator.scm.MultiEnvLatentSCM.push_forward--returns","text":"v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. Source code in data_generator/scm.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def push_forward ( self , u : Tensor , env : int ) -> Tensor : \"\"\" Push forward the latent variable u through the SCM in environment env. Parameters ---------- u: Tensor, shape (num_samples, latent_dim) Samples of the exogenous noise variables. env: int Environment index. Returns ------- v: Tensor, shape (num_samples, latent_dim) Samples of the latent variables. \"\"\" v = torch . nan * torch . zeros_like ( u ) for index in self . topological_order : f = self . functions_per_env [ env ][ index ] v [:, index ] = f ( v , u ) return v","title":"Returns"},{"location":"references/#model.cauca_model","text":"","title":"cauca_model"},{"location":"references/#model.cauca_model.CauCAModel","text":"Bases: LightningModule , ABC Base class for Causal Component Analysis (CauCA) models. It implements the training loop and the evaluation metrics.","title":"CauCAModel"},{"location":"references/#model.cauca_model.CauCAModel--attributes","text":"latent_dim : int Dimensionality of the latent space. adjacency_matrix : np.ndarray, shape (num_nodes, num_nodes) Adjacency matrix of the causal graph assumed by the model. This is not necessarily the true adjacency matrix of the data generating process (see below). adjacency_matrix_gt : np.ndarray, shape (num_nodes, num_nodes) Ground truth adjacency matrix of the causal graph. This is the adjacency matrix of the data generating process. adjacency_misspecified : bool Whether the adjacency matrix is misspecified. If True, the model assumes a wrong adjacency matrix. lr : float Learning rate for the optimizer. weight_decay : float Weight decay for the optimizer. lr_scheduler : str Learning rate scheduler to use. If None, no scheduler is used. Options are \"cosine\" or None. Default: None. lr_min : float Minimum learning rate for the scheduler. Default: 0.0. encoder : CauCAEncoder The CauCA encoder. Needs to be set in subclasses.","title":"Attributes"},{"location":"references/#model.cauca_model.CauCAModel--methods","text":"training_step(batch, batch_idx) -> Tensor Training step. validation_step(batch, batch_idx) -> dict[str, Tensor] Validation step: basically passes data to validation_epoch_end. validation_epoch_end(outputs) -> None Computes validation metrics across all validation data. test_step(batch, batch_idx) -> dict[str, Tensor] Test step: basically passes data to test_epoch_end. test_epoch_end(outputs) -> None Computes test metrics across all test data. configure_optimizers() -> dict | torch.optim.Optimizer Configures the optimizer and learning rate scheduler. forward(x) -> torch.Tensor Computes the latent variables from the observed data. on_before_optimizer_step(optimizer, optimizer_idx) -> None Callback that is called before each optimizer step. It ensures that some gradients are set to zero to fix some causal mechanisms. See documentation of ParamMultiEnvCausalDistribution for more details. set_adjacency(adjacency_matrix, adjacency_misspecified) -> np.ndarray Sets the adjacency matrix and possibly changes it if it is misspecified. Source code in model/cauca_model.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 class CauCAModel ( pl . LightningModule , ABC ): \"\"\" Base class for Causal Component Analysis (CauCA) models. It implements the training loop and the evaluation metrics. Attributes ---------- latent_dim : int Dimensionality of the latent space. adjacency_matrix : np.ndarray, shape (num_nodes, num_nodes) Adjacency matrix of the causal graph assumed by the model. This is not necessarily the true adjacency matrix of the data generating process (see below). adjacency_matrix_gt : np.ndarray, shape (num_nodes, num_nodes) Ground truth adjacency matrix of the causal graph. This is the adjacency matrix of the data generating process. adjacency_misspecified : bool Whether the adjacency matrix is misspecified. If True, the model assumes a wrong adjacency matrix. lr : float Learning rate for the optimizer. weight_decay : float Weight decay for the optimizer. lr_scheduler : str Learning rate scheduler to use. If None, no scheduler is used. Options are \"cosine\" or None. Default: None. lr_min : float Minimum learning rate for the scheduler. Default: 0.0. encoder : CauCAEncoder The CauCA encoder. Needs to be set in subclasses. Methods ------- training_step(batch, batch_idx) -> Tensor Training step. validation_step(batch, batch_idx) -> dict[str, Tensor] Validation step: basically passes data to validation_epoch_end. validation_epoch_end(outputs) -> None Computes validation metrics across all validation data. test_step(batch, batch_idx) -> dict[str, Tensor] Test step: basically passes data to test_epoch_end. test_epoch_end(outputs) -> None Computes test metrics across all test data. configure_optimizers() -> dict | torch.optim.Optimizer Configures the optimizer and learning rate scheduler. forward(x) -> torch.Tensor Computes the latent variables from the observed data. on_before_optimizer_step(optimizer, optimizer_idx) -> None Callback that is called before each optimizer step. It ensures that some gradients are set to zero to fix some causal mechanisms. See documentation of ParamMultiEnvCausalDistribution for more details. set_adjacency(adjacency_matrix, adjacency_misspecified) -> np.ndarray Sets the adjacency matrix and possibly changes it if it is misspecified. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , ) -> None : super () . __init__ () self . latent_dim = latent_dim self . adjacency_matrix = self . set_adjacency ( adjacency_matrix , adjacency_misspecified ) self . adjacency_matrix_gt = adjacency_matrix self . adjacency_misspecified = adjacency_misspecified self . lr = lr self . weight_decay = weight_decay self . lr_scheduler = lr_scheduler self . lr_min = lr_min self . encoder = None # needs to be set in subclasses self . save_hyperparameters () @staticmethod def set_adjacency ( adjacency_matrix : np . ndarray , adjacency_misspecified : bool ) -> np . ndarray : if not adjacency_misspecified : return adjacency_matrix if adjacency_matrix . shape [ 0 ] == 2 and np . sum ( adjacency_matrix ) == 0 : # for 2 variables, if adjacency matrix is [[0, 0], [0, 0]], then # replace with [[0, 1], [0, 0]] adjacency_matrix_out = np . zeros_like ( adjacency_matrix ) adjacency_matrix_out [ 0 , 1 ] = 1 return adjacency_matrix_out elif adjacency_matrix . shape [ 0 ] > 2 : raise ValueError ( \"Adjacency misspecification not supported for empty adjacency matrix for >2 variables\" ) else : return adjacency_matrix . T def training_step ( self , batch : tuple [ Tensor , ... ], batch_idx : int ) -> Tensor : x , v , u , e , int_target , log_prob_gt = batch log_prob , res = self . encoder . multi_env_log_prob ( x , e , int_target ) loss = - log_prob . mean () self . log ( \"train_loss\" , loss , prog_bar = False ) return loss def validation_step ( self , batch : tuple [ Tensor , ... ], batch_idx : int ) -> dict [ str , Tensor ]: x , v , u , e , int_target , log_prob_gt = batch log_prob , res = self . encoder . multi_env_log_prob ( x , e , int_target ) v_hat = self ( x ) return { \"log_prob\" : log_prob , \"log_prob_gt\" : log_prob_gt , \"v\" : v , \"v_hat\" : v_hat , } def validation_epoch_end ( self , outputs : List [ dict ]) -> None : log_prob = torch . cat ([ o [ \"log_prob\" ] for o in outputs ]) log_prob_gt = torch . cat ([ o [ \"log_prob_gt\" ] for o in outputs ]) v = torch . cat ([ o [ \"v\" ] for o in outputs ]) v_hat = torch . cat ([ o [ \"v_hat\" ] for o in outputs ]) mcc = mean_correlation_coefficient ( v_hat , v ) mcc_spearman = mean_correlation_coefficient ( v_hat , v , method = \"spearman\" ) loss = - log_prob . mean () loss_gt = - log_prob_gt . mean () self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_loss_gt\" , loss_gt , prog_bar = False ) self . log ( \"val_mcc\" , mcc . mean (), prog_bar = True ) for i , mcc_value in enumerate ( mcc ): self . log ( f \"val_mcc_ { i } \" , mcc_value , prog_bar = False ) self . log ( \"val_mcc_spearman\" , mcc_spearman . mean (), prog_bar = True ) for i , mcc_value in enumerate ( mcc_spearman ): self . log ( f \"val_mcc_spearman_ { i } \" , mcc_value , prog_bar = False ) def test_step ( self , batch : tuple [ Tensor , ... ], batch_idx : int ) -> Union [ None , dict [ str , Tensor ]]: x , v , u , e , int_target , log_prob_gt = batch log_prob , res = self . encoder . multi_env_log_prob ( x , e , int_target ) return { \"log_prob\" : log_prob , \"v\" : v , \"v_hat\" : self ( x ), } def test_epoch_end ( self , outputs : List [ dict ]) -> None : log_prob = torch . cat ([ o [ \"log_prob\" ] for o in outputs ]) loss = - log_prob . mean () v = torch . cat ([ o [ \"v\" ] for o in outputs ]) v_hat = torch . cat ([ o [ \"v_hat\" ] for o in outputs ]) mcc = mean_correlation_coefficient ( v_hat , v ) self . log ( \"test_loss\" , loss , prog_bar = False ) self . log ( \"test_mcc\" , mcc . mean (), prog_bar = False ) for i , mcc_value in enumerate ( mcc ): self . log ( f \"test_mcc_ { i } \" , mcc_value , prog_bar = False ) def configure_optimizers ( self ) -> dict | torch . optim . Optimizer : config_dict = {} optimizer = torch . optim . Adam ( self . parameters (), lr = self . lr , weight_decay = self . weight_decay ) config_dict [ \"optimizer\" ] = optimizer if self . lr_scheduler == \"cosine\" : # cosine learning rate annealing lr_scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = self . trainer . max_epochs , eta_min = self . lr_min , verbose = True , ) lr_scheduler_config = { \"scheduler\" : lr_scheduler , \"interval\" : \"epoch\" , } config_dict [ \"lr_scheduler\" ] = lr_scheduler_config elif self . lr_scheduler is None : return optimizer else : raise ValueError ( f \"Unknown lr_scheduler: { self . lr_scheduler } \" ) return config_dict def forward ( self , x : torch . Tensor ) -> torch . Tensor : v_hat = self . encoder ( x ) return v_hat def on_before_optimizer_step ( self , optimizer : Optimizer , optimizer_idx : int ) -> None : num_envs = len ( self . encoder . intervention_targets_per_env ) num_vars = self . adjacency_matrix . shape [ 0 ] # set gradients to fixed q0 parameters to zero if self . encoder . q0 . trainable : try : for param_idx , ( env , i ) in enumerate ( product ( range ( num_envs ), range ( num_vars )) ): if not self . encoder . q0 . noise_means_requires_grad [ env ][ i ]: list ( self . encoder . q0 . noise_means . parameters ())[ param_idx ] . grad = None if not self . encoder . q0 . noise_stds_requires_grad [ env ][ i ]: list ( self . encoder . q0 . noise_stds . parameters ())[ param_idx ] . grad = None except AttributeError : pass","title":"Methods"},{"location":"references/#model.cauca_model.LinearCauCAModel","text":"Bases: CauCAModel CauCA model with linear unmixing function. Source code in model/cauca_model.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 class LinearCauCAModel ( CauCAModel ): \"\"\" CauCA model with linear unmixing function. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , fix_mechanisms : bool = True , nonparametric_base_distr : bool = False , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , lr = lr , weight_decay = weight_decay , lr_scheduler = lr_scheduler , lr_min = lr_min , adjacency_misspecified = adjacency_misspecified , ) self . encoder = LinearCauCAEncoder ( latent_dim , self . adjacency_matrix , # this is the misspecified adjacency matrix if adjacency_misspecified=True intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , nonparametric_base_distr = nonparametric_base_distr , ) self . save_hyperparameters ()","title":"LinearCauCAModel"},{"location":"references/#model.cauca_model.NaiveNonlinearModel","text":"Bases: CauCAModel Naive CauCA model with nonlinear unmixing function. It assumes no causal dependencies. Source code in model/cauca_model.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 class NaiveNonlinearModel ( CauCAModel ): \"\"\" Naive CauCA model with nonlinear unmixing function. It assumes no causal dependencies. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , k_flows : int = 1 , intervention_targets_per_env : Optional [ torch . Tensor ] = None , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , lr = lr , weight_decay = weight_decay , lr_scheduler = lr_scheduler , lr_min = lr_min , adjacency_misspecified = adjacency_misspecified , ) self . encoder = NaiveEncoder ( latent_dim , self . adjacency_matrix , # this is the misspecified adjacency matrix if adjacency_misspecified=True K = k_flows , intervention_targets_per_env = intervention_targets_per_env , net_hidden_dim = net_hidden_dim , net_hidden_layers = net_hidden_layers , ) self . save_hyperparameters ()","title":"NaiveNonlinearModel"},{"location":"references/#model.cauca_model.NonlinearCauCAModel","text":"Bases: CauCAModel CauCA model with nonlinear unmixing function.","title":"NonlinearCauCAModel"},{"location":"references/#model.cauca_model.NonlinearCauCAModel--additional-attributes","text":"k_flows : int Number of flows to use in the nonlinear unmixing function. Default: 1. net_hidden_dim : int Hidden dimension of the neural network used in the nonlinear unmixing function. Default: 128. net_hidden_layers : int Number of hidden layers of the neural network used in the nonlinear unmixing function. Default: 3. fix_mechanisms : bool Some mechanisms can be fixed to a simple gaussian distribution without loss of generality. This has only an effect for the parametric base distribution. If True, these mechanisms are fixed. Default: True. fix_all_intervention_targets : bool When fixable mechanisms are fixed, this parameter determines whether all intervention targets are fixed (option 1) or all intervention targets which are non-root nodes together with all non-intervened root nodes (option 2). See documentation of ParamMultiEnvCausalDistribution for more details. Default: False. nonparametric_base_distr : bool Whether to use a nonparametric base distribution for the flows. If false, a parametric linear gaussian causal base distribution is used. Default: False. K_cbn : int Number of flows to use in the nonlinear nonparametric base distribution. Default: 3. net_hidden_dim_cbn : int Hidden dimension of the neural network used in the nonlinear nonparametric base distribution. Default: 128. net_hidden_layers_cbn : int Number of hidden layers of the neural network used in the nonlinear nonparametric base distribution. Default: 3. Source code in model/cauca_model.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class NonlinearCauCAModel ( CauCAModel ): \"\"\" CauCA model with nonlinear unmixing function. Additional attributes --------------------- k_flows : int Number of flows to use in the nonlinear unmixing function. Default: 1. net_hidden_dim : int Hidden dimension of the neural network used in the nonlinear unmixing function. Default: 128. net_hidden_layers : int Number of hidden layers of the neural network used in the nonlinear unmixing function. Default: 3. fix_mechanisms : bool Some mechanisms can be fixed to a simple gaussian distribution without loss of generality. This has only an effect for the parametric base distribution. If True, these mechanisms are fixed. Default: True. fix_all_intervention_targets : bool When fixable mechanisms are fixed, this parameter determines whether all intervention targets are fixed (option 1) or all intervention targets which are non-root nodes together with all non-intervened root nodes (option 2). See documentation of ParamMultiEnvCausalDistribution for more details. Default: False. nonparametric_base_distr : bool Whether to use a nonparametric base distribution for the flows. If false, a parametric linear gaussian causal base distribution is used. Default: False. K_cbn : int Number of flows to use in the nonlinear nonparametric base distribution. Default: 3. net_hidden_dim_cbn : int Hidden dimension of the neural network used in the nonlinear nonparametric base distribution. Default: 128. net_hidden_layers_cbn : int Number of hidden layers of the neural network used in the nonlinear nonparametric base distribution. Default: 3. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , lr : float = 1e-2 , weight_decay : float = 0 , lr_scheduler : Optional [ str ] = None , lr_min : float = 0.0 , adjacency_misspecified : bool = False , k_flows : int = 1 , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , fix_mechanisms : bool = True , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , K_cbn : int = 3 , net_hidden_dim_cbn : int = 128 , net_hidden_layers_cbn : int = 3 , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , lr = lr , weight_decay = weight_decay , lr_scheduler = lr_scheduler , lr_min = lr_min , adjacency_misspecified = adjacency_misspecified , ) self . encoder = NonlinearCauCAEncoder ( latent_dim , self . adjacency_matrix , # this is the misspecified adjacency matrix if adjacency_misspecified=True K = k_flows , intervention_targets_per_env = intervention_targets_per_env , net_hidden_dim = net_hidden_dim , net_hidden_layers = net_hidden_layers , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , nonparametric_base_distr = nonparametric_base_distr , K_cbn = K_cbn , net_hidden_dim_cbn = net_hidden_dim_cbn , net_hidden_layers_cbn = net_hidden_layers_cbn , ) self . save_hyperparameters ()","title":"Additional attributes"},{"location":"references/#model.encoder","text":"","title":"encoder"},{"location":"references/#model.encoder.CauCAEncoder","text":"Bases: NormalizingFlow CauCA encoder for multi-environment data. The encoder maps from the observed data x to the latent space v_hat. The latent space is assumed to have causal structure. The encoder is trained to maximize the likelihood of the data under the causal model. x and v_hat are assumed to have the same dimension. The encoder has two main components A causal base distribution q0 over the latent space. This encodes the latent causal structure. An unmixing function mapping from the observations to the latent space.","title":"CauCAEncoder"},{"location":"references/#model.encoder.CauCAEncoder--attributes","text":"latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. fix_mechanisms: bool Whether to fix some fixable mechanisms in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. fix_all_intervention_targets: bool Whether to fix all intervention targets in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. nonparametric_base_distr: bool Whether to use a nonparametric base distribution. If False, a parametric base distribution assuming linear causal mechanisms is used. Default: False. flows: Optional[list[nf.flows.Flow]] List of normalizing flows to use for the unmixing function. Default: None. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3.","title":"Attributes"},{"location":"references/#model.encoder.CauCAEncoder--methods","text":"multi_env_log_prob(x, e, intervention_targets) -> Tensor Computes log probability of x in environment e. forward(x) -> Tensor Maps from the observed data x to the latent space v_hat. Source code in model/encoder.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class CauCAEncoder ( nf . NormalizingFlow ): \"\"\" CauCA encoder for multi-environment data. The encoder maps from the observed data x to the latent space v_hat. The latent space is assumed to have causal structure. The encoder is trained to maximize the likelihood of the data under the causal model. x and v_hat are assumed to have the same dimension. The encoder has two main components: 1. A causal base distribution q0 over the latent space. This encodes the latent causal structure. 2. An unmixing function mapping from the observations to the latent space. Attributes ---------- latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. fix_mechanisms: bool Whether to fix some fixable mechanisms in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. fix_all_intervention_targets: bool Whether to fix all intervention targets in the causal model. (See documentation of the ParamMultiEnvCausalDistribution for details.) Default: False. nonparametric_base_distr: bool Whether to use a nonparametric base distribution. If False, a parametric base distribution assuming linear causal mechanisms is used. Default: False. flows: Optional[list[nf.flows.Flow]] List of normalizing flows to use for the unmixing function. Default: None. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. Methods ------- multi_env_log_prob(x, e, intervention_targets) -> Tensor Computes log probability of x in environment e. forward(x) -> Tensor Maps from the observed data x to the latent space v_hat. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Optional [ Tensor ] = None , fix_mechanisms : bool = False , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , flows : Optional [ list [ nf . flows . Flow ]] = None , q0 : Optional [ nf . distributions . BaseDistribution ] = None , K_cbn : int = 3 , net_hidden_dim_cbn : int = 128 , net_hidden_layers_cbn : int = 3 , ) -> None : self . latent_dim = latent_dim self . adjacency_matrix = adjacency_matrix self . intervention_targets_per_env = intervention_targets_per_env self . fix_mechanisms = fix_mechanisms self . fix_all_intervention_targets = fix_all_intervention_targets self . nonparametric_base_distr = nonparametric_base_distr self . K_cbn = K_cbn self . net_hidden_dim_cbn = net_hidden_dim_cbn self . net_hidden_layers_cbn = net_hidden_layers_cbn if q0 is None : if self . nonparametric_base_distr : q0 = NonparamMultiEnvCausalDistribution ( adjacency_matrix = adjacency_matrix , K = K_cbn , net_hidden_dim = net_hidden_dim_cbn , net_hidden_layers = net_hidden_layers_cbn , ) else : assert ( intervention_targets_per_env is not None ), \"intervention_targets_per_env must be provided for parametric base distribution\" q0 = ParamMultiEnvCausalDistribution ( adjacency_matrix = adjacency_matrix , intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , ) super () . __init__ ( q0 = q0 , flows = flows if flows is not None else []) def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : raise NotImplementedError def forward ( self , x : Tensor ) -> Tensor : raise NotImplementedError","title":"Methods"},{"location":"references/#model.encoder.LinearCauCAEncoder","text":"Bases: CauCAEncoder Linear CauCA encoder for multi-environment data. Source code in model/encoder.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class LinearCauCAEncoder ( CauCAEncoder ): \"\"\" Linear CauCA encoder for multi-environment data. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , intervention_targets_per_env : Optional [ Tensor ] = None , fix_mechanisms : bool = True , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , ) -> None : super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , nonparametric_base_distr = nonparametric_base_distr , ) self . unmixing = nn . Linear ( latent_dim , latent_dim , bias = False ) def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> tuple [ Tensor , dict [ str , Tensor ]]: v_hat = self ( x ) jacobian = torch . autograd . functional . jacobian ( self . unmixing , x [ 0 , :], create_graph = True ) log_q = torch . zeros ( len ( x ), dtype = x . dtype , device = x . device ) log_q += log ( abs ( det ( jacobian ))) determinant_terms = log_q prob_terms = self . q0 . multi_env_log_prob ( v_hat , e , intervention_targets ) log_q += prob_terms res = { \"log_prob\" : log_q , \"determinant_terms\" : determinant_terms , \"prob_terms\" : prob_terms , } return log_q , res def forward ( self , x : Tensor ) -> Tensor : latent = self . unmixing ( x ) return latent","title":"LinearCauCAEncoder"},{"location":"references/#model.encoder.NaiveEncoder","text":"Bases: NonlinearCauCAEncoder Naive encoder for multi-environment data. This encoder does not assume any causal structure in the latent space. Equivalent to independent components analysis (ICA). Source code in model/encoder.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class NaiveEncoder ( NonlinearCauCAEncoder ): \"\"\" Naive encoder for multi-environment data. This encoder does not assume any causal structure in the latent space. Equivalent to independent components analysis (ICA). \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , K : int = 1 , intervention_targets_per_env : Optional [ Tensor ] = None , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , ) -> None : # overwrite the q0 from NonlinearICAEncoder q0 = NaiveMultiEnvCausalDistribution ( adjacency_matrix = adjacency_matrix , ) super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , K = K , intervention_targets_per_env = intervention_targets_per_env , net_hidden_dim = net_hidden_dim , net_hidden_layers = net_hidden_layers , q0 = q0 , )","title":"NaiveEncoder"},{"location":"references/#model.encoder.NonlinearCauCAEncoder","text":"Bases: CauCAEncoder Nonlinear CauCA encoder for multi-environment data. Here the unmixing function is a normalizing flow.","title":"NonlinearCauCAEncoder"},{"location":"references/#model.encoder.NonlinearCauCAEncoder--parameters","text":"latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. K: int Number of normalizing flows to use for the unmixing function. Default: 1. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. net_hidden_dim: int Hidden dimension of the neural network used in the normalizing flows. Default: 128. net_hidden_layers: int Number of hidden layers in the neural network used in the normalizing flows. Default: 3. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. Source code in model/encoder.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 class NonlinearCauCAEncoder ( CauCAEncoder ): \"\"\" Nonlinear CauCA encoder for multi-environment data. Here the unmixing function is a normalizing flow. Parameters ---------- latent_dim: int Dimension of the latent and observed variables. adjacency_matrix: np.ndarray, shape (latent_dim, latent_dim) Adjacency matrix of the latent causal graph. K: int Number of normalizing flows to use for the unmixing function. Default: 1. intervention_targets_per_env: Tensor, shape (no_envs, latent_dim) Which variables are intervened on in each environment. net_hidden_dim: int Hidden dimension of the neural network used in the normalizing flows. Default: 128. net_hidden_layers: int Number of hidden layers in the neural network used in the normalizing flows. Default: 3. q0: Optional[nf.distributions.BaseDistribution] Base distribution over the latent space. Default: None. K_cbn: int Number of normalizing flows to use for the nonparametric base distribution. Default: 3. net_hidden_dim_cbn: int Hidden dimension of the neural network used in the nonparametric base distribution. Default: 128. net_hidden_layers_cbn: int Number of hidden layers in the neural network used in the nonparametric base distribution. Default: 3. \"\"\" def __init__ ( self , latent_dim : int , adjacency_matrix : np . ndarray , K : int = 1 , intervention_targets_per_env : Optional [ Tensor ] = None , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , fix_mechanisms : bool = True , fix_all_intervention_targets : bool = False , nonparametric_base_distr : bool = False , q0 : Optional [ nf . distributions . BaseDistribution ] = None , K_cbn : int = 3 , net_hidden_dim_cbn : int = 128 , net_hidden_layers_cbn : int = 3 , ) -> None : self . K = K self . intervention_targets_per_env = intervention_targets_per_env self . net_hidden_dim = net_hidden_dim self . net_hidden_layers = net_hidden_layers flows = make_spline_flows ( K , latent_dim , net_hidden_dim , net_hidden_layers ) super () . __init__ ( latent_dim = latent_dim , adjacency_matrix = adjacency_matrix , intervention_targets_per_env = intervention_targets_per_env , fix_mechanisms = fix_mechanisms , fix_all_intervention_targets = fix_all_intervention_targets , nonparametric_base_distr = nonparametric_base_distr , flows = flows , q0 = q0 , K_cbn = K_cbn , net_hidden_dim_cbn = net_hidden_dim_cbn , net_hidden_layers_cbn = net_hidden_layers_cbn , ) def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> tuple [ Tensor , dict [ str , Tensor ]]: log_q = torch . zeros ( len ( x ), dtype = x . dtype , device = x . device ) z = x for i in range ( len ( self . flows ) - 1 , - 1 , - 1 ): z , log_det = self . flows [ i ] . inverse ( z ) log_q += log_det determinant_terms = log_q prob_terms = self . q0 . multi_env_log_prob ( z , e , intervention_targets ) log_q += prob_terms res = { \"log_prob\" : log_q , \"determinant_terms\" : determinant_terms , \"prob_terms\" : prob_terms , } return log_q , res def forward ( self , x : Tensor ) -> Tensor : return self . inverse ( x )","title":"Parameters"},{"location":"references/#model.normalizing_flow","text":"","title":"normalizing_flow"},{"location":"references/#model.normalizing_flow.distribution","text":"","title":"distribution"},{"location":"references/#model.normalizing_flow.distribution.MultiEnvCausalDistribution","text":"Bases: BaseDistribution , ABC Base class for parametric multi-environment causal distributions. In typical normalizing flow architectures, the base distribution is a simple distribution such as a multivariate Gaussian. In our case, the base distribution has additional multi-environment causal structure. Hence, in the parametric case, this class learns the parameters of the causal mechanisms and noise distributions. The causal graph is assumed to be known. This is a subclass of BaseDistribution, which is a subclass of torch.nn.Module. Hence, this class can be used as a base distribution in a normalizing flow.","title":"MultiEnvCausalDistribution"},{"location":"references/#model.normalizing_flow.distribution.MultiEnvCausalDistribution--methods","text":"multi_env_log_prob(z, e, intervention_targets) -> Tensor Compute the log probability of the latent variables v in environment e, given the intervention targets. This is used as the main training objective. Source code in model/normalizing_flow/distribution.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MultiEnvCausalDistribution ( nf . distributions . BaseDistribution , ABC ): \"\"\" Base class for parametric multi-environment causal distributions. In typical normalizing flow architectures, the base distribution is a simple distribution such as a multivariate Gaussian. In our case, the base distribution has additional multi-environment causal structure. Hence, in the parametric case, this class learns the parameters of the causal mechanisms and noise distributions. The causal graph is assumed to be known. This is a subclass of BaseDistribution, which is a subclass of torch.nn.Module. Hence, this class can be used as a base distribution in a normalizing flow. Methods ------- multi_env_log_prob(z, e, intervention_targets) -> Tensor Compute the log probability of the latent variables v in environment e, given the intervention targets. This is used as the main training objective. \"\"\" def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : raise NotImplementedError","title":"Methods"},{"location":"references/#model.normalizing_flow.distribution.NaiveMultiEnvCausalDistribution","text":"Bases: MultiEnvCausalDistribution Naive multi-environment causal distribution. This is a dummy-version of ParamMultiEnvCausalDistribution, where the causal mechanisms are assumed to be trivial (no connectioons between variables) and the noise distributions are assumed to be Gaussian and independent of the environment. This is equivalent to the independent component analysis (ICA) case. Source code in model/normalizing_flow/distribution.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 class NaiveMultiEnvCausalDistribution ( MultiEnvCausalDistribution ): \"\"\" Naive multi-environment causal distribution. This is a dummy-version of ParamMultiEnvCausalDistribution, where the causal mechanisms are assumed to be trivial (no connectioons between variables) and the noise distributions are assumed to be Gaussian and independent of the environment. This is equivalent to the independent component analysis (ICA) case. \"\"\" def __init__ ( self , adjacency_matrix : np . ndarray , ) -> None : super () . __init__ () self . adjacency_matrix = adjacency_matrix self . q0 = DiagGaussian ( adjacency_matrix . shape [ 0 ], trainable = True ) def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : return self . q0 . log_prob ( z )","title":"NaiveMultiEnvCausalDistribution"},{"location":"references/#model.normalizing_flow.distribution.ParamMultiEnvCausalDistribution","text":"Bases: MultiEnvCausalDistribution Parametric multi-environment causal distribution. This class learns the parameters of the causal mechanisms and noise distributions. The causal mechanisms are assumed to be linear, and the noise distributions are assumed to be Gaussian. In environments where a variable is intervened on, the connection to its parents is assumed to be cut off, and the noise distribution can be shifted relative to the observational environment (when the variable is not intervened on). Theoretically, we can fix some of the mechanisms involved w.l.o.g. and still achieve identifiability (see Appendix G2 of [1]). There are two ways to do this: 1. Fix all mechanisms that are intervened on. 2. Fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set. However, we do not have to fix any of the mechanisms and in practice, we find that this leads to better performance.","title":"ParamMultiEnvCausalDistribution"},{"location":"references/#model.normalizing_flow.distribution.ParamMultiEnvCausalDistribution--attributes","text":"adjacency_matrix: np.ndarray Adjacency matrix of the causal graph. fix_mechanisms: bool Whether to fix any of the mechanisms. Default: False. fix_all_intervention_targets: bool Whether to fix all mechanisms that are intervened on (option 1 above). If False, we fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set (option 2 above). Default: False. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. dag: nx.DiGraph Directed acyclic graph of the causal connections. coeff_values: nn.ParameterList List of lists of coefficients for the linear mechanisms. The outer list has length equal to the number of variables, and the inner list has length equal to the number of parents of the variable. The last element of the inner list is the variance parameter. I.e. coeff_values i are the linear weights of the parent variables of variable i, and coeff_values i is weight of the exogenous noise. noise_means: nn.ParameterList List of lists of means for the noise distributions. The outer list has length equal to the number of environments, and the inner list has length equal to the number of variables. noise_means e is the mean of the noise distribution for variable i in environment e. Note that not all of these parameters are used in the computation of the log probability. If a variable i is not intervened on in environment e, we use the observational noise distribution, i.e. noise_means 0 (e=0 is assumed to be the observational environment). noise_stds: nn.ParameterList Same as noise_means, but for the standard deviations of the noise distributions. coeff_values_requires_grad: list[list[bool]] Whether each coefficient is trainable. This is used to fix the coefficients of the mechanisms. noise_means_requires_grad: list[list[bool]] Whether each noise mean is trainable. This is used to fix the noise means of the mechanisms. noise_stds_requires_grad: list[list[bool]] Whether each noise standard deviation is trainable. This is used to fix the noise standard deviations","title":"Attributes"},{"location":"references/#model.normalizing_flow.distribution.ParamMultiEnvCausalDistribution--references","text":"[1] https://arxiv.org/abs/2305.17225 Source code in model/normalizing_flow/distribution.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class ParamMultiEnvCausalDistribution ( MultiEnvCausalDistribution ): \"\"\" Parametric multi-environment causal distribution. This class learns the parameters of the causal mechanisms and noise distributions. The causal mechanisms are assumed to be linear, and the noise distributions are assumed to be Gaussian. In environments where a variable is intervened on, the connection to its parents is assumed to be cut off, and the noise distribution can be shifted relative to the observational environment (when the variable is not intervened on). Theoretically, we can fix some of the mechanisms involved w.l.o.g. and still achieve identifiability (see Appendix G2 of [1]). There are two ways to do this: 1. Fix all mechanisms that are intervened on. 2. Fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set. However, we do not have to fix any of the mechanisms and in practice, we find that this leads to better performance. Attributes ---------- adjacency_matrix: np.ndarray Adjacency matrix of the causal graph. fix_mechanisms: bool Whether to fix any of the mechanisms. Default: False. fix_all_intervention_targets: bool Whether to fix all mechanisms that are intervened on (option 1 above). If False, we fix all observational mechanisms with an empty parent set and all intervened mechanisms with a non-empty parent set (option 2 above). Default: False. intervention_targets_per_env: Tensor, shape (num_envs, latent_dim) Intervention targets per environment, with 1 indicating that the variable is intervened on and 0 indicating that the variable is not intervened on. This variable also implicitly defines the number of environments. dag: nx.DiGraph Directed acyclic graph of the causal connections. coeff_values: nn.ParameterList List of lists of coefficients for the linear mechanisms. The outer list has length equal to the number of variables, and the inner list has length equal to the number of parents of the variable. The last element of the inner list is the variance parameter. I.e. coeff_values[i][:-1] are the linear weights of the parent variables of variable i, and coeff_values[i][-1] is weight of the exogenous noise. noise_means: nn.ParameterList List of lists of means for the noise distributions. The outer list has length equal to the number of environments, and the inner list has length equal to the number of variables. noise_means[e][i] is the mean of the noise distribution for variable i in environment e. Note that not all of these parameters are used in the computation of the log probability. If a variable i is not intervened on in environment e, we use the observational noise distribution, i.e. noise_means[0][i] (e=0 is assumed to be the observational environment). noise_stds: nn.ParameterList Same as noise_means, but for the standard deviations of the noise distributions. coeff_values_requires_grad: list[list[bool]] Whether each coefficient is trainable. This is used to fix the coefficients of the mechanisms. noise_means_requires_grad: list[list[bool]] Whether each noise mean is trainable. This is used to fix the noise means of the mechanisms. noise_stds_requires_grad: list[list[bool]] Whether each noise standard deviation is trainable. This is used to fix the noise standard deviations References ---------- [1] https://arxiv.org/abs/2305.17225 \"\"\" trainable = True def __init__ ( self , adjacency_matrix : np . ndarray , intervention_targets_per_env : Tensor , fix_mechanisms : bool = False , fix_all_intervention_targets : bool = False , ) -> None : super () . __init__ () self . adjacency_matrix = adjacency_matrix self . fix_mechanisms = fix_mechanisms self . fix_all_intervention_targets = fix_all_intervention_targets self . intervention_targets_per_env = intervention_targets_per_env self . dag = nx . DiGraph ( adjacency_matrix ) device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) coeff_values , coeff_values_requires_grad = self . _set_initial_coeffs ( self . dag , device ) noise_means , noise_means_requires_grad = self . _set_initial_noise_means ( self . dag , fix_mechanisms , intervention_targets_per_env , fix_all_intervention_targets , device , ) noise_stds , noise_stds_requires_grad = self . _set_initial_noise_stds ( self . dag , fix_mechanisms , intervention_targets_per_env , fix_all_intervention_targets , device , ) self . coeff_values = nn . ParameterList ( coeff_values ) self . noise_means = nn . ParameterList ( noise_means ) self . noise_stds = nn . ParameterList ( noise_stds ) self . coeff_values_requires_grad = coeff_values_requires_grad self . noise_means_requires_grad = noise_means_requires_grad self . noise_stds_requires_grad = noise_stds_requires_grad def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : log_p = torch . zeros ( len ( z ), dtype = z . dtype , device = z . device ) for env in e . unique (): env_mask = ( e == env ) . flatten () z_env = z [ env_mask , :] intervention_targets_env = intervention_targets [ env_mask , :] for i in range ( z . shape [ 1 ]): parents = list ( self . dag . predecessors ( i )) if len ( parents ) == 0 or intervention_targets_env [ 0 , i ] == 1 : parent_contribution = 0 else : coeffs_raw = self . coeff_values [ i ][: - 1 ] if isinstance ( coeffs_raw , nn . ParameterList ): coeffs_raw = torch . cat ([ c for c in coeffs_raw ]) parent_coeffs = coeffs_raw . to ( z . device ) parent_contribution = parent_coeffs . matmul ( z_env [:, parents ] . T ) noise_env_idx = int ( env ) if intervention_targets_env [ 0 , i ] == 1 else 0 var = self . noise_stds [ noise_env_idx ][ i ] ** 2 * torch . ones_like ( z_env [:, i ] ) noise_coeff = self . coeff_values [ i ][ - 1 ] . to ( z . device ) noise_contribution = noise_coeff * self . noise_means [ noise_env_idx ][ i ] var *= noise_coeff ** 2 log_p [ env_mask ] += torch . distributions . Normal ( parent_contribution + noise_contribution , var . sqrt () ) . log_prob ( z_env [:, i ]) return log_p @staticmethod def _set_initial_coeffs ( dag : nx . DiGraph , device : torch . device ) -> tuple [ list [ ParameterList ], list [ list [ bool ]]]: coeff_values = [] coeff_values_requires_grad = [] for i in range ( dag . number_of_nodes ()): coeff_values_i = [] coeff_values_requires_grad_i = [] num_parents = len ( list ( dag . predecessors ( i ))) for j in range ( num_parents ): random_val = Uniform ( - 1 , 1 ) . sample (( 1 ,)) val = random_val param = nn . Parameter ( val * torch . ones ( 1 ), requires_grad = True ) . to ( device ) coeff_values_i . append ( param ) coeff_values_requires_grad_i . append ( True ) const = torch . ones ( 1 , requires_grad = False ) . to ( device ) # variance param coeff_values_i . append ( const ) coeff_values_requires_grad_i . append ( False ) coeff_values . append ( nn . ParameterList ( coeff_values_i )) coeff_values_requires_grad . append ( coeff_values_requires_grad_i ) return coeff_values , coeff_values_requires_grad @staticmethod def _set_initial_noise_means ( dag : nx . DiGraph , fix_mechanisms : bool , intervention_targets_per_env : Tensor , fix_all_intervention_targets : bool , device : torch . device , ) -> tuple [ list [ ParameterList ], list [ list [ bool ]]]: noise_means = [] noise_means_requires_grad = [] num_envs = intervention_targets_per_env . shape [ 0 ] for e in range ( num_envs ): noise_means_e = [] noise_means_requires_grad_e = [] for i in range ( dag . number_of_nodes ()): is_shifted = intervention_targets_per_env [ e ][ i ] == 1 is_root = len ( list ( dag . predecessors ( i ))) == 0 if fix_all_intervention_targets : is_fixed = is_shifted else : is_fixed = ( is_shifted and not is_root ) or ( not is_shifted and is_root ) is_fixed = is_fixed and fix_mechanisms random_val = Uniform ( - 0.5 , 0.5 ) . sample (( 1 ,)) val = random_val param = ( nn . Parameter ( val * torch . ones ( 1 ), requires_grad = not is_fixed ) ) . to ( device ) noise_means_e . append ( param ) noise_means_requires_grad_e . append ( not is_fixed ) noise_means . append ( nn . ParameterList ( noise_means_e )) noise_means_requires_grad . append ( noise_means_requires_grad_e ) return noise_means , noise_means_requires_grad @staticmethod def _set_initial_noise_stds ( dag : nx . DiGraph , fix_mechanisms : bool , intervention_targets_per_env : Tensor , fix_all_intervention_targets : bool , device : torch . device , ) -> tuple [ list [ ParameterList ], list [ list [ bool ]]]: noise_stds = [] noise_stds_requires_grad = [] for e in range ( intervention_targets_per_env . shape [ 0 ]): noise_stds_e = [] noise_stds_requires_grad_e = [] for i in range ( dag . number_of_nodes ()): is_shifted = intervention_targets_per_env [ e ][ i ] == 1 is_root = len ( list ( dag . predecessors ( i ))) == 0 if fix_all_intervention_targets : is_fixed = is_shifted else : is_fixed = ( is_shifted and not is_root ) or ( not is_shifted and is_root ) is_fixed = is_fixed and fix_mechanisms random_val = Uniform ( 0.5 , 1.5 ) . sample (( 1 ,)) val = random_val param = ( nn . Parameter ( val * torch . ones ( 1 ), requires_grad = not is_fixed ) ) . to ( device ) noise_stds_e . append ( param ) noise_stds_requires_grad_e . append ( not is_fixed ) noise_stds . append ( nn . ParameterList ( noise_stds_e )) noise_stds_requires_grad . append ( noise_stds_requires_grad_e ) return noise_stds , noise_stds_requires_grad","title":"References"},{"location":"references/#model.normalizing_flow.nonparametric_distribution","text":"","title":"nonparametric_distribution"},{"location":"references/#model.normalizing_flow.nonparametric_distribution.MultiEnvBaseDistribution","text":"Bases: BaseDistribution Base distribution for nonparametric multi-environment causal distributions. This simple independent Gaussian distribution is used as the base distribution for the nonparametric multi-environment causal distribution. I.e. this distribution represents the exogenous noise in the SCM. Source code in model/normalizing_flow/nonparametric_distribution.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class MultiEnvBaseDistribution ( nf . distributions . BaseDistribution ): \"\"\" Base distribution for nonparametric multi-environment causal distributions. This simple independent Gaussian distribution is used as the base distribution for the nonparametric multi-environment causal distribution. I.e. this distribution represents the exogenous noise in the SCM. \"\"\" def multi_env_log_prob ( self , x : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : gaussian_nll = gaussian_nll_loss ( x , torch . zeros_like ( x ), torch . ones_like ( x ), full = True , reduction = \"none\" ) mask = ~ intervention_targets . to ( bool ) log_p = - ( mask * gaussian_nll ) . sum ( dim = 1 ) return log_p","title":"MultiEnvBaseDistribution"},{"location":"references/#model.normalizing_flow.nonparametric_distribution.NonparamMultiEnvCausalDistribution","text":"Bases: NormalizingFlow Nonarametric multi-environment causal distribution. A nonparametric causal distribution that uses a normalizing flow to parameterize the latent causal mechanisms. This causal distribution has two parts: 1. The latent SCM, which is parameterized by a normalizing flow. It represents the reduced form of the SCM, mapping independent (Gaussian) exogenous noise to the endogenous latent variables. The causal structure of the latent SCM is encoded through the topological order of the latent variables according to the adjacency matrix. 2. Fixed, simple base distributions for the mechanisms that are intervened on.","title":"NonparamMultiEnvCausalDistribution"},{"location":"references/#model.normalizing_flow.nonparametric_distribution.NonparamMultiEnvCausalDistribution--attributes","text":"adjacency_matrix : np.ndarray The adjacency matrix of the SCM. K : int The number of normalizing flow blocks to use for the reduced form of the SCM. net_hidden_dim : int The hidden dimension of the neural networks used in the normalizing flow blocks. net_hidden_layers : int The number of hidden layers of the neural networks used in the normalizing flow blocks. perm : torch.Tensor The permutation of the latent variables according to the topological order.","title":"Attributes"},{"location":"references/#model.normalizing_flow.nonparametric_distribution.NonparamMultiEnvCausalDistribution--methods","text":"multi_env_log_prob(z, e, intervention_targets) -> torch.Tensor Compute the log probability of the given data.","title":"Methods"},{"location":"references/#model.normalizing_flow.nonparametric_distribution.NonparamMultiEnvCausalDistribution--references","text":"[1] https://arxiv.org/abs/2305.17225 Source code in model/normalizing_flow/nonparametric_distribution.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class NonparamMultiEnvCausalDistribution ( nf . NormalizingFlow ): \"\"\" Nonarametric multi-environment causal distribution. A nonparametric causal distribution that uses a normalizing flow to parameterize the latent causal mechanisms. This causal distribution has two parts: 1. The latent SCM, which is parameterized by a normalizing flow. It represents the reduced form of the SCM, mapping independent (Gaussian) exogenous noise to the endogenous latent variables. The causal structure of the latent SCM is encoded through the topological order of the latent variables according to the adjacency matrix. 2. Fixed, simple base distributions for the mechanisms that are intervened on. Attributes ---------- adjacency_matrix : np.ndarray The adjacency matrix of the SCM. K : int The number of normalizing flow blocks to use for the reduced form of the SCM. net_hidden_dim : int The hidden dimension of the neural networks used in the normalizing flow blocks. net_hidden_layers : int The number of hidden layers of the neural networks used in the normalizing flow blocks. perm : torch.Tensor The permutation of the latent variables according to the topological order. Methods ------- multi_env_log_prob(z, e, intervention_targets) -> torch.Tensor Compute the log probability of the given data. References ---------- [1] https://arxiv.org/abs/2305.17225 \"\"\" trainable = True def __init__ ( self , adjacency_matrix : np . ndarray , K : int = 3 , net_hidden_dim : int = 128 , net_hidden_layers : int = 3 , ) -> None : self . adjacency_matrix = adjacency_matrix self . K = K self . net_hidden_dim = net_hidden_dim self . net_hidden_layers = net_hidden_layers latent_dim = adjacency_matrix . shape [ 0 ] # permutation according to topological order self . perm = torch . tensor ( list ( nx . topological_sort ( nx . DiGraph ( self . adjacency_matrix ))), dtype = torch . long , ) flows = make_spline_flows ( K , latent_dim , net_hidden_dim , net_hidden_layers , permutation = False ) q0 = MultiEnvBaseDistribution () super () . __init__ ( q0 = q0 , flows = flows ) def multi_env_log_prob ( self , z : Tensor , e : Tensor , intervention_targets : Tensor ) -> Tensor : z = z [:, self . perm ] # permute inputs to be in topological order log_q , u = self . _determinant_terms ( intervention_targets , z ) prob_terms = self . q0 . multi_env_log_prob ( u , e , intervention_targets ) prob_terms_intervened = self . _prob_terms_intervened ( intervention_targets , z ) log_q += prob_terms + prob_terms_intervened return log_q def _determinant_terms ( self , intervention_targets : Tensor , z : Tensor ) -> tuple [ Tensor , Tensor ]: log_q = torch . zeros ( len ( z ), dtype = z . dtype , device = z . device ) u = z for i in range ( len ( self . flows ) - 1 , - 1 , - 1 ): u , log_det = self . flows [ i ] . inverse ( u ) log_q += log_det # remove determinant terms for intervened mechanisms jac_row = torch . autograd . functional . jvp ( self . inverse , z , v = intervention_targets , create_graph = True )[ 1 ] jac_diag_element = ( jac_row * intervention_targets ) . sum ( dim = 1 ) # mask zero elements not_intervened_mask = ~ intervention_targets . sum ( dim = 1 ) . to ( bool ) jac_diag_element [ not_intervened_mask ] = 1 log_q -= log ( abs ( jac_diag_element ) + 1e-8 ) return log_q , u def _prob_terms_intervened ( self , intervention_targets : Tensor , z : Tensor ) -> Tensor : \"\"\" Compute the probability terms for the intervened mechanisms. \"\"\" gaussian_nll = gaussian_nll_loss ( z , torch . zeros_like ( z ), torch . ones_like ( z ), full = True , reduction = \"none\" ) mask = intervention_targets . to ( bool ) prob_terms_intervention_targets = - ( mask * gaussian_nll ) . sum ( dim = 1 ) return prob_terms_intervention_targets","title":"References"}]}